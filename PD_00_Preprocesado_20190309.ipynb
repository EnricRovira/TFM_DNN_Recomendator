{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.kdnuggets.com/wp-content/uploads/text-preprocessing-framework-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import contractions\n",
    "import inflect\n",
    "import re, unicodedata\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, \\\n",
    "                          UpSampling1D, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#import my_functions\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_ESP = list(set(stopwords.words('spanish')))\n",
    "STOPWORDS_CAT = open('stopwords_catalan.txt', 'r', encoding= 'UTF-8').read().split()\n",
    "STOPWORDS_ENG = list(set(stopwords.words('english')))\n",
    "CUSTOM  = ['uf', '100st', '100t', '100u', 'pal', 'zzb21601xu', 'x2']\n",
    "\n",
    "STOPWORDS_ALL = STOPWORDS_ESP + STOPWORDS_CAT + STOPWORDS_ENG + CUSTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file download with wget module\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-es.txt'  \n",
    "#wget.download(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = {}\n",
    "with open(\"lemmatization-es.txt\", encoding= 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            (key, val) = line.split()\n",
    "        except:\n",
    "            pass\n",
    "        lemmatizer[str(key)] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_inv = {v: k for k, v in lemmatizer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../02_Comprension de Datos/Descripciones204k_20190315.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo masculino con textura de mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                                          text  \n",
       "0                     Moda Mujer Abrigos Abrigo masculino con textura de mujer  \n",
       "1             Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono  \n",
       "2   Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés  \n",
       "3  Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos  \n",
       "4            Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una lista de marcas para poder quitarlas del texto en caso de que aparezcan, eliminamos las marcas duplicadas para comprimir el tamaño de la lista y optimizar el proceso de bsuqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                        1025942232486\n",
       "brand                                          Fox_Home\n",
       "text     Cine Series de Tv Futurama. 7ª Temporada (DVD)\n",
       "Name: 199000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[199000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "copia = data.copy()\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def normalize_brands(words):\n",
    "    words = str(words)\n",
    "    words  = words.strip().split()\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)    \n",
    "    words = list(dict.fromkeys(words)) #Remove duplicates\n",
    "    words = \" \".join(words)\n",
    "    return words\n",
    "    \n",
    "copia['brand'] = copia['brand'].apply(normalize_brands)\n",
    "brands = list(dict.fromkeys(copia['brand'].values))\n",
    "brands.remove('marvel')\n",
    "more_brands = ['tarocco', 'tucci', 'fjord', 'tommy', 'hilfiger', 'easy', 'wear', 'josma', 'jaipur', 'ralph', 'nemeziz']\n",
    "for brand in more_brands:\n",
    "    brands.append(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5091"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la normalización vamos a realizar una serie de tareas destinadas a poner todo el texto en un campo de juego nivelado: convertir todo el texto a la misma mayúscula o minúscula, eliminar la puntuación, convertir las cifras a sus equivalentes en palabras, etc. La normalización pone todas las palabras en pie de igualdad y permite que el procesamiento se realice de manera uniforme. Algunas de las técnicas que vamos a aplicar son:\n",
    "\n",
    "- Borrar caracteres extraños\n",
    "- Pasar todo el texto a minusculas\n",
    "- Quitar simbolos de puntuacion ( . , &, !, ?, ¿, /, etc)\n",
    "- Pasar numeros en digito a texto\n",
    "- Quitar stopwords\n",
    "- Stemming\n",
    "- Lexematizar\n",
    "\n",
    "**Importante:** Despues de la Nomralizacion trabajaremos a nivel de palabra o token en vez de a nivel de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ['corte ingles', 'bob esponja', 'color azul', 'cristiano ronaldo', 'fc barcelona', 'lionel messi', 'real madrid',\n",
    "         'azul oscuro', '1a temporada', '2a temporada', '3a temporada', '4a temporada', '5a temporada', '6a temporada',\n",
    "         '7a temporada', '8a temporada', 'mrs increible', 'mr increible', 'fc barcelona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        #new_word = unicodedata.normalize('NFD', word).encode('ascii', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if not word.isdigit():\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORDS_ALL and len(word) > 1:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def remove_brands(words):\n",
    "    \"\"\"Remove brands from the text\"\"\"\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        if word not in brands:\n",
    "            clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "def lemmatize(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        if word in lemmatizer_inv:\n",
    "            lemmas.append(lemmatizer_inv[word])\n",
    "        else: lemmas.append(word)\n",
    "    return lemmas\n",
    "\n",
    "def replace_ngrams(words):\n",
    "    for gram in ngrams:\n",
    "        if (gram in words):\n",
    "            g = \"_\".join(gram.split())\n",
    "            words = words.replace(gram, g)\n",
    "    return words\n",
    "\n",
    "def normalize(words):\n",
    "    #if pandas == True:\n",
    "    words  = words.strip().split()\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_brands(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = list(dict.fromkeys(words)) #Remove duplicates\n",
    "    words = \" \".join(words)\n",
    "    words = replace_ngrams(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#4min 4sec\n",
    "data_copy['text'] = data_copy['text'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo masculino textura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo doble faz cinturon tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo largo antelina woman corte_ingles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>moda mujer abrigo chaqueta termica efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>moda mujer abrigo parka algodon capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                     text  \n",
       "0                     moda mujer abrigo masculino textura  \n",
       "1               moda mujer abrigo doble faz cinturon tono  \n",
       "2     moda mujer abrigo largo antelina woman corte_ingles  \n",
       "3  moda mujer abrigo chaqueta termica efecto cortavientos  \n",
       "4                 moda mujer abrigo parka algodon capucha  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo masculino con textura de mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                                          text  \n",
       "0                     Moda Mujer Abrigos Abrigo masculino con textura de mujer  \n",
       "1             Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono  \n",
       "2   Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés  \n",
       "3  Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos  \n",
       "4            Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                        1025942232486\n",
      "brand                                          Fox_Home\n",
      "text     Cine Series de Tv Futurama. 7ª Temporada (DVD)\n",
      "Name: 199000, dtype: object\n",
      "id                                       1014970203072\n",
      "brand                                Polo_Ralph_Lauren\n",
      "text     moda mujer punto jersey lana polo cuello caja\n",
      "Name: 89000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#7000\n",
    "print(data.loc[199000])\n",
    "print(data_copy.loc[89000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204812,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max words in a sentence:          Mean words in a sentence:\n",
      "------------------------------------------------------------\n",
      "23                                8.059176220143351\n",
      "------------------------------------------------------------\n",
      "libro salud bienestar dieta nutricion general detox sen sano dentro bello clave nutricional rutina diarias eliminar toxina forma saludable energetica nutritiva tapa blanda\n"
     ]
    }
   ],
   "source": [
    "maxw = 0\n",
    "mean = 0\n",
    "max_s = 0\n",
    "for i, sentence in enumerate(data_copy['text']):\n",
    "    mean += len(sentence.split())\n",
    "    if maxw < len(sentence.split()):\n",
    "        maxw = len(sentence.split())\n",
    "        max_s = i\n",
    "print('Max words in a sentence:' + ' '*10 + 'Mean words in a sentence:')\n",
    "print(\"-\" * 60)\n",
    "print (f\"{maxw} {' '*30} {mean/len(data_copy['text'])}\")\n",
    "print(\"-\" * 60)\n",
    "print (data_copy['text'][max_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.to_csv('Texto_PreProcesado.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramas (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'moda', 'mujer', 'abrigo', 'masculino', 'textura', '1', 'doble', 'faz', 'cinturon']\n",
      "257064\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = nltk.word_tokenize(data_copy['text'].to_string())\n",
    "bag_of_words = list(dict.fromkeys(bag_of_words))\n",
    "print(bag_of_words[:10])\n",
    "print(len(bag_of_words)) \n",
    "#126k productos = 2_063_845 palabras\n",
    "#175k productos = 2_838_097 palabras\n",
    "#204k productos = 3_070_773 palabras -> 1_878_308 20/03/2019 -> no dups = 257042, ngrams =257063 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moda mujer abrigo masculino textura',\n",
       " 'moda mujer abrigo doble faz cinturon tono',\n",
       " 'moda mujer abrigo largo antelina woman corte_ingles',\n",
       " 'moda mujer abrigo chaqueta termica efecto cortavientos',\n",
       " 'moda mujer abrigo parka algodon capucha']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [sent for sent in data_copy['text']]\n",
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moda',\n",
       " 'mujer',\n",
       " 'abrigo',\n",
       " 'masculino',\n",
       " 'textura',\n",
       " 'moda mujer',\n",
       " 'mujer abrigo',\n",
       " 'abrigo masculino',\n",
       " 'masculino textura',\n",
       " 'doble']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.keys())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['moda', 'mujer', 'abrigo', 'masculino', 'textura'],\n",
       " ['moda', 'mujer', 'abrigo', 'doble', 'faz', 'cinturon', 'tono'],\n",
       " ['moda', 'mujer', 'abrigo', 'largo', 'antelina', 'woman', 'corte_ingles']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [word.split() for word in data_copy['text'].values]\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204812"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelWV = Word2Vec(sentences, workers = 3, min_count=5, window = 10, size = EMBEDDING_DIM)\n",
    "modelWV.train(sentences, total_examples=len(sentences), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWV.save(\"word2vec_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alpargata', 0.6336812973022461),\n",
       " ('botin', 0.6093630790710449),\n",
       " ('salon', 0.6039133071899414),\n",
       " ('nautico', 0.5760550498962402),\n",
       " ('bota', 0.5563323497772217),\n",
       " ('chancla', 0.5541836619377136),\n",
       " ('mocasin', 0.5491766929626465),\n",
       " ('pepito', 0.5403501391410828),\n",
       " ('mercedita', 0.5259843468666077),\n",
       " ('deportivas', 0.5120893716812134)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Word2Vec.load(\"word2vec_model_v1\")\n",
    "wl = 'sandalia'\n",
    "modelWV.wv.most_similar (positive = wl)\n",
    "#model.wv.most_similar_cosmul(positive = ['disfraz', 'abrigo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maternal', 0.46743133664131165),\n",
       " ('cambiador', 0.465241014957428),\n",
       " ('seat', 0.45979371666908264),\n",
       " ('carrycot', 0.4566614329814911),\n",
       " ('cochesilla', 0.4491623044013977),\n",
       " ('portabebes', 0.44005948305130005),\n",
       " ('capazo', 0.4304969608783722),\n",
       " ('born', 0.4299528896808624),\n",
       " ('trona', 0.4292840361595154),\n",
       " ('isofix', 0.4285898804664612)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similar_products(text):\n",
    "    text = normalize(text)\n",
    "    list_text = text.split()\n",
    "    most_similar = modelWV.wv.most_similar_cosmul(positive = list_text)\n",
    "    \n",
    "    return most_similar\n",
    "    \n",
    "similar_products('Silla de paseo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Sentences\n",
    "\n",
    "- Initialize tokenizer with num_words = MAX_NB_WORDS (200K). i.e. The tokenizer will perform a word count, sorted by number of occurences in descending order and pick top N words, 200K in this case \n",
    "- Use tokenizer's texts_to_sequences method to convert text to array of integers.\n",
    "- The arrays obtained from previous step might not be of uniform length, use pad_sequences method to obtain arrays with length equal to MAX_SEQUENCE_LENGTH (30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = len(bag_of_words) #200k\n",
    "MAX_SEQUENCE_LENGTH = 24                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = data_copy['text']\n",
    "all_text = all_text.drop_duplicates (keep = False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "data_sequences = tokenizer.texts_to_sequences(data_copy['text'])\n",
    "data_vec = pad_sequences(data_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49418 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#49421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moda': 1,\n",
       " 'accesorio': 2,\n",
       " 'mujer': 3,\n",
       " 'bebe': 4,\n",
       " 'infantil': 5,\n",
       " 'hombre': 6,\n",
       " 'cine': 7,\n",
       " 'hogar': 8,\n",
       " 'azul': 9,\n",
       " 'libro': 10,\n",
       " 'electronica': 11,\n",
       " 'deporte': 12,\n",
       " 'musica': 13,\n",
       " 'nina': 14,\n",
       " 'tapa': 15,\n",
       " 'dvd': 16,\n",
       " 'negro': 17,\n",
       " 'corte': 18,\n",
       " 'juguete': 19,\n",
       " 'nino': 20,\n",
       " 'ingles': 21,\n",
       " 'perfumeria': 22,\n",
       " 'mes': 23,\n",
       " 'color': 24,\n",
       " 'regalo': 25,\n",
       " 'ano': 26,\n",
       " 'cd': 27,\n",
       " 'zapato': 28,\n",
       " 'original': 29,\n",
       " 'blanda': 30,\n",
       " 'piel': 31,\n",
       " 'juego': 32,\n",
       " 'gris': 33,\n",
       " 'ropa': 34,\n",
       " 'papeleria': 35,\n",
       " 'rosa': 36,\n",
       " 'menaje': 37,\n",
       " 'electrodomestico': 38,\n",
       " 'blanco': 39,\n",
       " 'bolso': 40,\n",
       " 'cocina': 41,\n",
       " 'estampado': 42,\n",
       " 'mesa': 43,\n",
       " 'camiseta': 44,\n",
       " 'silla': 45,\n",
       " 'novela': 46,\n",
       " 'zapatilla': 47,\n",
       " 'bluray': 48,\n",
       " 'pantalon': 49,\n",
       " 'paseo': 50,\n",
       " 'bano': 51,\n",
       " 'pack': 52,\n",
       " 'rostro': 53,\n",
       " 'coche': 54,\n",
       " 'bisuteria': 55,\n",
       " 'decoracion': 56,\n",
       " 'reloj': 57,\n",
       " 'jersey': 58,\n",
       " 'chaqueta': 59,\n",
       " 'punto': 60,\n",
       " 'rojo': 61,\n",
       " 'camisa': 62,\n",
       " 'manga': 63,\n",
       " 'marino': 64,\n",
       " 'marron': 65,\n",
       " 'cuidado': 66,\n",
       " 'general': 67,\n",
       " 'funda': 68,\n",
       " 'joyeria': 69,\n",
       " 'verde': 70,\n",
       " 'textil': 71,\n",
       " 'mini': 72,\n",
       " 'vestido': 73,\n",
       " 'polo': 74,\n",
       " 'deportiva': 75,\n",
       " 'maquillaje': 76,\n",
       " 'mochila': 77,\n",
       " 'videojuego': 78,\n",
       " 'parafarmacia': 79,\n",
       " 'plata': 80,\n",
       " 'raya': 81,\n",
       " 'mascota': 82,\n",
       " 'negra': 83,\n",
       " 'lenceria': 84,\n",
       " 'figura': 85,\n",
       " 'grande': 86,\n",
       " 'clasico': 87,\n",
       " 'dura': 88,\n",
       " 'algodon': 89,\n",
       " 'cama': 90,\n",
       " 'multicolor': 91,\n",
       " 'tratamiento': 92,\n",
       " 'beige': 93,\n",
       " 'abrigo': 94,\n",
       " 'cartera': 95,\n",
       " 'movil': 96,\n",
       " 'bandolera': 97,\n",
       " 'comic': 98,\n",
       " 'complemento': 99,\n",
       " 'cuerpo': 100,\n",
       " 'tuc': 101,\n",
       " 'carro': 102,\n",
       " 'bota': 103,\n",
       " 'pulsera': 104,\n",
       " 'mililitro': 105,\n",
       " 'crema': 106,\n",
       " 'cremallera': 107,\n",
       " 'star': 108,\n",
       " 'baby': 109,\n",
       " 'cotton': 110,\n",
       " 'hidratante': 111,\n",
       " 'mueble': 112,\n",
       " 'blusa': 113,\n",
       " 'peluche': 114,\n",
       " 'top': 115,\n",
       " 'set': 116,\n",
       " 'sonido': 117,\n",
       " 'sol': 118,\n",
       " 'sudadera': 119,\n",
       " 'equipaje': 120,\n",
       " 'lpvinilo': 121,\n",
       " 'capacidad': 122,\n",
       " 'gafas': 123,\n",
       " 'pop': 124,\n",
       " 'cuello': 125,\n",
       " 'flor': 126,\n",
       " 'escritura': 127,\n",
       " 'portatil': 128,\n",
       " 'espanola': 129,\n",
       " 'texto': 130,\n",
       " 'neceser': 131,\n",
       " 'espanol': 132,\n",
       " 'muneca': 133,\n",
       " 'muneco': 134,\n",
       " 'pieza': 135,\n",
       " 'serie': 136,\n",
       " 'rock': 137,\n",
       " 'home': 138,\n",
       " 'deportivas': 139,\n",
       " 'gorro': 140,\n",
       " 'vehiculo': 141,\n",
       " 'placa': 142,\n",
       " 'acero': 143,\n",
       " 'auto': 144,\n",
       " 'ojo': 145,\n",
       " 'pendiente': 146,\n",
       " 'monedero': 147,\n",
       " 'campana': 148,\n",
       " 'cuna': 149,\n",
       " 'cuadro': 150,\n",
       " 'new': 151,\n",
       " 'oro': 152,\n",
       " 'tv': 153,\n",
       " 'horno': 154,\n",
       " 'detalle': 155,\n",
       " 'pijama': 156,\n",
       " 'fiesta': 157,\n",
       " 'bolsa': 158,\n",
       " 'futbol': 159,\n",
       " 'jardin': 160,\n",
       " 'cordon': 161,\n",
       " 'corporal': 162,\n",
       " 'corta': 163,\n",
       " 'articulado': 164,\n",
       " 'preescolar': 165,\n",
       " 'juice': 166,\n",
       " 'manir': 167,\n",
       " 'escritorio': 168,\n",
       " 'maleta': 169,\n",
       " 'wars': 170,\n",
       " 'aventura': 171,\n",
       " 'pelo': 172,\n",
       " 'narrativa': 173,\n",
       " 'unisex': 174,\n",
       " 'western': 175,\n",
       " 'perro': 176,\n",
       " 'metal': 177,\n",
       " 'grupo': 178,\n",
       " 'recambio': 179,\n",
       " 'hombro': 180,\n",
       " 'calcetin': 181,\n",
       " 'juvenil': 182,\n",
       " 'larga': 183,\n",
       " 'playa': 184,\n",
       " 'lectura': 185,\n",
       " 'bordado': 186,\n",
       " 'higiene': 187,\n",
       " 'vaquero': 188,\n",
       " 'playsets': 189,\n",
       " 'preparacion': 190,\n",
       " 'natural': 191,\n",
       " 'mundo': 192,\n",
       " 'internacional': 193,\n",
       " 'cancion': 194,\n",
       " 'one': 195,\n",
       " 'caja': 196,\n",
       " 'pro': 197,\n",
       " 'banda': 198,\n",
       " 'black': 199,\n",
       " 'casual': 200,\n",
       " 'estuche': 201,\n",
       " 'cuaderno': 202,\n",
       " 'ps4': 203,\n",
       " 'viaje': 204,\n",
       " 'alimento': 205,\n",
       " 'doble': 206,\n",
       " 'frigorifico': 207,\n",
       " 'congelador': 208,\n",
       " 'ciencia': 209,\n",
       " 'plus': 210,\n",
       " 'dia': 211,\n",
       " 'facial': 212,\n",
       " 'amarillo': 213,\n",
       " 'entretenimiento': 214,\n",
       " 'capucha': 215,\n",
       " 'falda': 216,\n",
       " 'musical': 217,\n",
       " 'salud': 218,\n",
       " 'classic': 219,\n",
       " 'libre': 220,\n",
       " 'gel': 221,\n",
       " 'accion': 222,\n",
       " 'emidio': 223,\n",
       " 'bienestar': 224,\n",
       " 'gato': 225,\n",
       " 'panuelo': 226,\n",
       " 'plato': 227,\n",
       " 'colgante': 228,\n",
       " 'corto': 229,\n",
       " 'informatica': 230,\n",
       " 'taza': 231,\n",
       " 'fular': 232,\n",
       " 'sombrero': 233,\n",
       " 'antiedad': 234,\n",
       " 'drama': 235,\n",
       " 'dieta': 236,\n",
       " 'bolsillo': 237,\n",
       " 'carcasa': 238,\n",
       " 'autoayuda': 239,\n",
       " 'print': 240,\n",
       " 'blanca': 241,\n",
       " 'joven': 242,\n",
       " 'botin': 243,\n",
       " 'age': 244,\n",
       " 'boton': 245,\n",
       " 'agua': 246,\n",
       " 'oscuro': 247,\n",
       " 'pc': 248,\n",
       " 'talla': 249,\n",
       " 'pequena': 250,\n",
       " 'agenda': 251,\n",
       " 'fragancia': 252,\n",
       " 'romantica': 253,\n",
       " 'capazo': 254,\n",
       " 'joe': 255,\n",
       " 'confort': 256,\n",
       " 'saco': 257,\n",
       " 'bluetooth': 258,\n",
       " 'protector': 259,\n",
       " 'historica': 260,\n",
       " 'calendario': 261,\n",
       " 'auricular': 262,\n",
       " 'body': 263,\n",
       " 'pelicula': 264,\n",
       " 'bicolor': 265,\n",
       " 'vela': 266,\n",
       " 'primera': 267,\n",
       " 'asa': 268,\n",
       " 'eau': 269,\n",
       " 'cierre': 270,\n",
       " 'formula': 271,\n",
       " 'montana': 272,\n",
       " 'regular': 273,\n",
       " 'frontal': 274,\n",
       " 'slim': 275,\n",
       " 'volante': 276,\n",
       " 'terror': 277,\n",
       " 'lujo': 278,\n",
       " 'collar': 279,\n",
       " 'temporada': 280,\n",
       " 'interior': 281,\n",
       " 'policiaca': 282,\n",
       " 'documental': 283,\n",
       " 'milimetro': 284,\n",
       " 'topo': 285,\n",
       " 'regional': 286,\n",
       " 'cazadora': 287,\n",
       " 'recomendadas': 288,\n",
       " 'sandalia': 289,\n",
       " 'cinturon': 290,\n",
       " 'dorado': 291,\n",
       " 'comedia': 292,\n",
       " 'intriga': 293,\n",
       " 'suspense': 294,\n",
       " 'estrella': 295,\n",
       " 'fantastico': 296,\n",
       " 'modelo': 297,\n",
       " 'liso': 298,\n",
       " 'do': 299,\n",
       " 'aroma': 300,\n",
       " 'impresora': 301,\n",
       " 'largo': 302,\n",
       " 'ordenador': 303,\n",
       " 'sabana': 304,\n",
       " 'rigida': 305,\n",
       " 'limpieza': 306,\n",
       " 'sonoras': 307,\n",
       " 'vaso': 308,\n",
       " 'poprock': 309,\n",
       " 'jeans': 310,\n",
       " 'ficcion': 311,\n",
       " 'leggins': 312,\n",
       " 'pan': 313,\n",
       " 'smartphones': 314,\n",
       " 'hop': 315,\n",
       " 'jazz': 316,\n",
       " 'infancia': 317,\n",
       " 'naranja': 318,\n",
       " 'conjunto': 319,\n",
       " 'libreta': 320,\n",
       " 'animacion': 321,\n",
       " 'braga': 322,\n",
       " 'dance': 323,\n",
       " 'puzzle': 324,\n",
       " 'soulfunkrb': 325,\n",
       " 'jazzblues': 326,\n",
       " 'piccolo': 327,\n",
       " 'disfraz': 328,\n",
       " 'duo': 329,\n",
       " 'tutto': 330,\n",
       " 'multifuncion': 331,\n",
       " 'ordenacion': 332,\n",
       " 'chocolate': 333,\n",
       " 'cabello': 334,\n",
       " 'neck': 335,\n",
       " 'led': 336,\n",
       " 'correccion': 337,\n",
       " 'estampada': 338,\n",
       " 'encaje': 339,\n",
       " 'claro': 340,\n",
       " 'anime': 341,\n",
       " 'floral': 342,\n",
       " 'lisa': 343,\n",
       " 'logo': 344,\n",
       " 'running': 345,\n",
       " 'romantico': 346,\n",
       " 'corbata': 347,\n",
       " 'tinta': 348,\n",
       " 'hip': 349,\n",
       " 'hard': 350,\n",
       " 'bermuda': 351,\n",
       " 'construccion': 352,\n",
       " 'rap': 353,\n",
       " 'edicion': 354,\n",
       " 'green': 355,\n",
       " 'rinonera': 356,\n",
       " 'cojin': 357,\n",
       " 'frikis': 358,\n",
       " 'transparente': 359,\n",
       " 'aire': 360,\n",
       " 'bermudas': 361,\n",
       " 'altavoz': 362,\n",
       " 'blazer': 363,\n",
       " 'esqui': 364,\n",
       " 'peto': 365,\n",
       " 'mediana': 366,\n",
       " 'puerta': 367,\n",
       " 'minicunas': 368,\n",
       " 'educativo': 369,\n",
       " 'house': 370,\n",
       " 'pequeno': 371,\n",
       " 'cafe': 372,\n",
       " 'collection': 373,\n",
       " 'ferreteria': 374,\n",
       " 'klein': 375,\n",
       " 'padel': 376,\n",
       " 'calvin': 377,\n",
       " 'short': 378,\n",
       " 'noche': 379,\n",
       " 'tecnologia': 380,\n",
       " 'ranita': 381,\n",
       " 'belico': 382,\n",
       " 'superheroes': 383,\n",
       " 'anillo': 384,\n",
       " 'roja': 385,\n",
       " 'pepe': 386,\n",
       " 'mejor': 387,\n",
       " 'perla': 388,\n",
       " 'kg': 389,\n",
       " 'casa': 390,\n",
       " 'gorra': 391,\n",
       " 'anual': 392,\n",
       " 'capilar': 393,\n",
       " 'antiarruga': 394,\n",
       " 'madrid': 395,\n",
       " 'hd': 396,\n",
       " 'invierno': 397,\n",
       " 'redondo': 398,\n",
       " 'digital': 399,\n",
       " 'snow': 400,\n",
       " 'hispanoamericana': 401,\n",
       " 'media': 402,\n",
       " 'coleccion': 403,\n",
       " 'corazon': 404,\n",
       " 'bicicleta': 405,\n",
       " 'hoja': 406,\n",
       " 'pantalla': 407,\n",
       " 'iphone': 408,\n",
       " 'iluminacion': 409,\n",
       " 'salon': 410,\n",
       " 'vida': 411,\n",
       " 'copa': 412,\n",
       " 'super': 413,\n",
       " 'granate': 414,\n",
       " 'ortiz': 415,\n",
       " 'boligrafo': 416,\n",
       " 'coast': 417,\n",
       " 'papel': 418,\n",
       " 'solapa': 419,\n",
       " 'te': 420,\n",
       " 'fitness': 421,\n",
       " 'real': 422,\n",
       " 'sujetador': 423,\n",
       " 'comida': 424,\n",
       " 'madera': 425,\n",
       " 'paris': 426,\n",
       " 'calzado': 427,\n",
       " 'shopping': 428,\n",
       " 'xbox': 429,\n",
       " 'ensamblaje': 430,\n",
       " 'compositor': 431,\n",
       " 'tipo': 432,\n",
       " 'minicuna': 433,\n",
       " 'animal': 434,\n",
       " 'primaria': 435,\n",
       " 'tapizado': 436,\n",
       " 'panties': 437,\n",
       " 'instrumento': 438,\n",
       " 'ii': 439,\n",
       " 'luz': 440,\n",
       " 'kit': 441,\n",
       " 'cambiador': 442,\n",
       " 'banador': 443,\n",
       " 'blue': 444,\n",
       " 'celeste': 445,\n",
       " 'adulto': 446,\n",
       " 'extranjera': 447,\n",
       " '3d': 448,\n",
       " 'modulares': 449,\n",
       " 'artistico': 450,\n",
       " 'exterior': 451,\n",
       " 'venir': 452,\n",
       " 'contraste': 453,\n",
       " 'lana': 454,\n",
       " 'disco': 455,\n",
       " 'wonderful': 456,\n",
       " 'radiocontrol': 457,\n",
       " 'aro': 458,\n",
       " 'love': 459,\n",
       " 'reversible': 460,\n",
       " 'cabina': 461,\n",
       " 'acuatico': 462,\n",
       " 'north': 463,\n",
       " 'galaxy': 464,\n",
       " 'harry': 465,\n",
       " 'edition': 466,\n",
       " 'parka': 467,\n",
       " 'vacuna': 468,\n",
       " 'preparar': 469,\n",
       " 'escote': 470,\n",
       " 'toalla': 471,\n",
       " 'ligeras': 472,\n",
       " 'deportivo': 473,\n",
       " 'silicona': 474,\n",
       " 'especial': 475,\n",
       " 'usb': 476,\n",
       " 'control': 477,\n",
       " 'habilidad': 478,\n",
       " 'labio': 479,\n",
       " 'face': 480,\n",
       " 'burdeos': 481,\n",
       " 'camel': 482,\n",
       " 'capota': 483,\n",
       " 'correa': 484,\n",
       " 'urban': 485,\n",
       " 'circuito': 486,\n",
       " 'potter': 487,\n",
       " 'tres': 488,\n",
       " 'switch': 489,\n",
       " 'adorno': 490,\n",
       " 'memoria': 491,\n",
       " 'piscina': 492,\n",
       " 'barra': 493,\n",
       " 'aceite': 494,\n",
       " 'bluchers': 495,\n",
       " 'polipiel': 496,\n",
       " 'monitor': 497,\n",
       " 'manta': 498,\n",
       " 'efecto': 499,\n",
       " 'lateral': 500,\n",
       " 'mayor': 501,\n",
       " 'champus': 502,\n",
       " 'champu': 503,\n",
       " 'equipaciones': 504,\n",
       " 'smart': 505,\n",
       " 'pie': 506,\n",
       " 'rectangular': 507,\n",
       " 'bufanda': 508,\n",
       " 'latino': 509,\n",
       " 'carga': 510,\n",
       " 'gaming': 511,\n",
       " 'mocasin': 512,\n",
       " 'ssd': 513,\n",
       " 'lavadora': 514,\n",
       " 'ultra': 515,\n",
       " 'diadema': 516,\n",
       " 'vista': 517,\n",
       " 'decorativas': 518,\n",
       " 'full': 519,\n",
       " 'terraza': 520,\n",
       " 'sinfonico': 521,\n",
       " 'city': 522,\n",
       " 'country': 523,\n",
       " 'cartucho': 524,\n",
       " 'trona': 525,\n",
       " 'emporio': 526,\n",
       " 'cafetera': 527,\n",
       " 'dependencia': 528,\n",
       " '1a': 529,\n",
       " 'sombra': 530,\n",
       " 'ortopedia': 531,\n",
       " 'tarjeta': 532,\n",
       " 'fotografia': 533,\n",
       " 'objetivo': 534,\n",
       " 'pico': 535,\n",
       " 'decorativa': 536,\n",
       " 'techno': 537,\n",
       " 'woman': 538,\n",
       " 'ducha': 539,\n",
       " 'mascara': 540,\n",
       " 'r': 541,\n",
       " 'p': 542,\n",
       " 'm': 543,\n",
       " 'kaki': 544,\n",
       " 'llavero': 545,\n",
       " '4k': 546,\n",
       " 'toilette': 547,\n",
       " 'air': 548,\n",
       " 'isize': 549,\n",
       " 'nylon': 550,\n",
       " 'bailarinas': 551,\n",
       " 'morado': 552,\n",
       " 'teenstore': 553,\n",
       " 'jugar': 554,\n",
       " 'tarjetero': 555,\n",
       " 'coral': 556,\n",
       " 'duro': 557,\n",
       " 'locion': 558,\n",
       " 'ciclismo': 559,\n",
       " 'premium': 560,\n",
       " 'velcro': 561,\n",
       " 'aluminio': 562,\n",
       " 'combi': 563,\n",
       " 'little': 564,\n",
       " 'redonda': 565,\n",
       " 'pen': 566,\n",
       " 'chasis': 567,\n",
       " 'manual': 568,\n",
       " 'fucsia': 569,\n",
       " 'roller': 570,\n",
       " 'sport': 571,\n",
       " 'grey': 572,\n",
       " 'marvel': 573,\n",
       " 'sephora': 574,\n",
       " 'vertical': 575,\n",
       " 'malla': 576,\n",
       " 'wifi': 577,\n",
       " 'pasito': 578,\n",
       " 'encuadernadas': 579,\n",
       " 'diseno': 580,\n",
       " 'cinta': 581,\n",
       " 'chino': 582,\n",
       " 'lampara': 583,\n",
       " 'roberto': 584,\n",
       " 'dorada': 585,\n",
       " 'chandals': 586,\n",
       " 'no': 587,\n",
       " 'tono': 588,\n",
       " 'drive': 589,\n",
       " 'pluma': 590,\n",
       " 'mascarilla': 591,\n",
       " 'tejido': 592,\n",
       " 'guante': 593,\n",
       " 'equipacion': 594,\n",
       " 'botella': 595,\n",
       " 'alto': 596,\n",
       " 'sobreponer': 597,\n",
       " 'piedra': 598,\n",
       " 'goma': 599,\n",
       " 'americana': 600,\n",
       " 'cadena': 601,\n",
       " 'luna': 602,\n",
       " 'portadocumentos': 603,\n",
       " 'mantel': 604,\n",
       " 'plateado': 605,\n",
       " 'parfum': 606,\n",
       " 'mano': 607,\n",
       " 'leotardo': 608,\n",
       " 'historia': 609,\n",
       " 'southern': 610,\n",
       " 'serraje': 611,\n",
       " 'plegable': 612,\n",
       " 'bluegrass': 613,\n",
       " 'texmex': 614,\n",
       " 'vaquera': 615,\n",
       " 'lente': 616,\n",
       " 'tierra': 617,\n",
       " 'studio': 618,\n",
       " 'cataluna': 619,\n",
       " 'ajustable': 620,\n",
       " 'cf': 621,\n",
       " 'espiral': 622,\n",
       " 'turquesa': 623,\n",
       " 'tirante': 624,\n",
       " 'traje': 625,\n",
       " 'bso': 626,\n",
       " 'lino': 627,\n",
       " 'maletin': 628,\n",
       " 'extractoras': 629,\n",
       " 'bajo': 630,\n",
       " 'molly': 631,\n",
       " 'lunar': 632,\n",
       " 'tote': 633,\n",
       " 'montura': 634,\n",
       " 'dc': 635,\n",
       " 'dragon': 636,\n",
       " 'colchoneta': 637,\n",
       " 'delantero': 638,\n",
       " 'tricot': 639,\n",
       " 'tenis': 640,\n",
       " 'vol': 641,\n",
       " 'lapiz': 642,\n",
       " 'nordica': 643,\n",
       " 'a4': 644,\n",
       " 'plumifero': 645,\n",
       " 'raton': 646,\n",
       " 'petit': 647,\n",
       " 'eye': 648,\n",
       " 'slippers': 649,\n",
       " 'robot': 650,\n",
       " 'electronico': 651,\n",
       " 'boch': 652,\n",
       " 'postre': 653,\n",
       " 'verino': 654,\n",
       " 'panty': 655,\n",
       " 'amor': 656,\n",
       " 'villeroy': 657,\n",
       " 'life': 658,\n",
       " '2a': 659,\n",
       " 'fantasia': 660,\n",
       " 'go': 661,\n",
       " 'marca': 662,\n",
       " 'paraguas': 663,\n",
       " 'cuero': 664,\n",
       " 'pelele': 665,\n",
       " 'auxiliares': 666,\n",
       " 'pique': 667,\n",
       " 'basic': 668,\n",
       " 'oficial': 669,\n",
       " 'induccion': 670,\n",
       " 'rueda': 671,\n",
       " 'semana': 672,\n",
       " 'padre': 673,\n",
       " 'bass': 674,\n",
       " 'zona': 675,\n",
       " 'desodorante': 676,\n",
       " 'gastronomia': 677,\n",
       " 'mono': 678,\n",
       " 'exclusivo': 679,\n",
       " 'bikini': 680,\n",
       " 'circonitas': 681,\n",
       " 'camara': 682,\n",
       " 'room': 683,\n",
       " 'adolfo': 684,\n",
       " 'dominguez': 685,\n",
       " 'pitillo': 686,\n",
       " 'reacondicionados': 687,\n",
       " 'princesa': 688,\n",
       " 'lona': 689,\n",
       " 'cristal': 690,\n",
       " 'light': 691,\n",
       " 'barcelona': 692,\n",
       " 'alfombra': 693,\n",
       " 'contorno': 694,\n",
       " 'mickey': 695,\n",
       " 'hebilla': 696,\n",
       " 'elastico': 697,\n",
       " 'pinza': 698,\n",
       " 'linea': 699,\n",
       " 'mediano': 700,\n",
       " 'reacondicionado': 701,\n",
       " 'grabado': 702,\n",
       " 'golf': 703,\n",
       " 'autor': 704,\n",
       " 'oso': 705,\n",
       " 'silver': 706,\n",
       " 'bateau': 707,\n",
       " 'yves': 708,\n",
       " 'loreal': 709,\n",
       " 'estilo': 710,\n",
       " 'convertible': 711,\n",
       " 'compartimento': 712,\n",
       " 'rodio': 713,\n",
       " 'pompon': 714,\n",
       " 'tela': 715,\n",
       " 'jacquard': 716,\n",
       " 'material': 717,\n",
       " 'gold': 718,\n",
       " 'deluxe': 719,\n",
       " 'fc': 720,\n",
       " 'gran': 721,\n",
       " 'crudo': 722,\n",
       " 'acetato': 723,\n",
       " 'combinado': 724,\n",
       " 'grafica': 725,\n",
       " 'pared': 726,\n",
       " 'solares': 727,\n",
       " 'acolchada': 728,\n",
       " 'mate': 729,\n",
       " 'spray': 730,\n",
       " 'comedor': 731,\n",
       " 'limpiadoras': 732,\n",
       " 'chill': 733,\n",
       " 'nude': 734,\n",
       " 'archivo': 735,\n",
       " 'nordicas': 736,\n",
       " 'recibidor': 737,\n",
       " 'multilingue': 738,\n",
       " 'proteccion': 739,\n",
       " 'sofa': 740,\n",
       " 'spf': 741,\n",
       " 'portatiles': 742,\n",
       " 'cepillo': 743,\n",
       " 'precio': 744,\n",
       " 'jabon': 745,\n",
       " 'pestana': 746,\n",
       " 'saint': 747,\n",
       " 'look': 748,\n",
       " 'jack': 749,\n",
       " 'mostaza': 750,\n",
       " 'gr': 751,\n",
       " 'serum': 752,\n",
       " 'lentejuela': 753,\n",
       " 'michael': 754,\n",
       " 'kors': 755,\n",
       " 'nutricion': 756,\n",
       " 'vinilo': 757,\n",
       " 'junior': 758,\n",
       " 'comunion': 759,\n",
       " 'inalambrico': 760,\n",
       " 'pintado': 761,\n",
       " 'visual': 762,\n",
       " 'colecho': 763,\n",
       " 'iii': 764,\n",
       " 'ambient': 765,\n",
       " 'tacon': 766,\n",
       " 'live': 767,\n",
       " 'aspiracion': 768,\n",
       " 'cover': 769,\n",
       " 'power': 770,\n",
       " 'club': 771,\n",
       " 'verano': 772,\n",
       " 'soporte': 773,\n",
       " 'termo': 774,\n",
       " 'rotulador': 775,\n",
       " 'happy': 776,\n",
       " 'convencional': 777,\n",
       " 'men': 778,\n",
       " 'pure': 779,\n",
       " 'lavavajillas': 780,\n",
       " '8gb': 781,\n",
       " 'practica': 782,\n",
       " 'extraible': 783,\n",
       " 'litro': 784,\n",
       " 'jugador': 785,\n",
       " 'dibujo': 786,\n",
       " 'electrico': 787,\n",
       " 'reposteria': 788,\n",
       " 'classics': 789,\n",
       " 'coco': 790,\n",
       " 'enviar': 791,\n",
       " 'moises': 792,\n",
       " 'cintura': 793,\n",
       " 'punta': 794,\n",
       " 'vajilla': 795,\n",
       " 'completo': 796,\n",
       " 'partir': 797,\n",
       " 'lite': 798,\n",
       " 'bella': 799,\n",
       " 'kids': 800,\n",
       " 'extensible': 801,\n",
       " 'almohada': 802,\n",
       " 'estrenar': 803,\n",
       " 'cuchillo': 804,\n",
       " 'sillon': 805,\n",
       " 'cortina': 806,\n",
       " 'metalico': 807,\n",
       " 'fashion': 808,\n",
       " 'tablet': 809,\n",
       " 'leche': 810,\n",
       " 'base': 811,\n",
       " 'vestir': 812,\n",
       " 'oficina': 813,\n",
       " 'lila': 814,\n",
       " 'pena': 815,\n",
       " 'porta': 816,\n",
       " 'radio': 817,\n",
       " 'blister': 818,\n",
       " 'britax': 819,\n",
       " 'romer': 820,\n",
       " 'balsamo': 821,\n",
       " 'individual': 822,\n",
       " 'microfono': 823,\n",
       " 'reflex': 824,\n",
       " 'una': 825,\n",
       " 'geometrico': 826,\n",
       " 'embarazo': 827,\n",
       " 'clasificacion': 828,\n",
       " 'basico': 829,\n",
       " 'alta': 830,\n",
       " 'fleco': 831,\n",
       " 'book': 832,\n",
       " 'tb': 833,\n",
       " 'chaleco': 834,\n",
       " 'pasta': 835,\n",
       " 'primero': 836,\n",
       " 'max': 837,\n",
       " 'split': 838,\n",
       " 'plancha': 839,\n",
       " 'agatha': 840,\n",
       " 'hdd': 841,\n",
       " 'cable': 842,\n",
       " 'circonita': 843,\n",
       " 'acolchado': 844,\n",
       " 'nafnaf': 845,\n",
       " 'hobo': 846,\n",
       " 'ball': 847,\n",
       " 'brocha': 848,\n",
       " 'reggae': 849,\n",
       " 'hilo': 850,\n",
       " 'run': 851,\n",
       " 'walking': 852,\n",
       " 'polvo': 853,\n",
       " 'parte': 854,\n",
       " 'vapor': 855,\n",
       " 'charm': 856,\n",
       " 'inverter': 857,\n",
       " 'popular': 858,\n",
       " 'guitarra': 859,\n",
       " 'nacido': 860,\n",
       " '3a': 861,\n",
       " 'fina': 862,\n",
       " 'broche': 863,\n",
       " 'royal': 864,\n",
       " 'equipo': 865,\n",
       " 'exchange': 866,\n",
       " 'pana': 867,\n",
       " 'ingle': 868,\n",
       " 'compacto': 869,\n",
       " 'world': 870,\n",
       " 'pensar': 871,\n",
       " 'evil': 872,\n",
       " 'melange': 873,\n",
       " 'music': 874,\n",
       " 'carpeta': 875,\n",
       " 'fluido': 876,\n",
       " 'cat': 877,\n",
       " 'mando': 878,\n",
       " 'acondicionado': 879,\n",
       " 'literatura': 880,\n",
       " 'cuadrada': 881,\n",
       " 'vidal': 882,\n",
       " 'templado': 883,\n",
       " 'frigh': 884,\n",
       " 'secreto': 885,\n",
       " 'capsula': 886,\n",
       " 'forma': 887,\n",
       " 'sobremesa': 888,\n",
       " 'skin': 889,\n",
       " 'cajon': 890,\n",
       " 'lifting': 891,\n",
       " 'dub': 892,\n",
       " 'automatico': 893,\n",
       " 'recien': 894,\n",
       " 'armour': 895,\n",
       " 'tacha': 896,\n",
       " 'lurex': 897,\n",
       " 'fred': 898,\n",
       " 'tricolor': 899,\n",
       " 'limitada': 900,\n",
       " 'varilla': 901,\n",
       " 'pili': 902,\n",
       " 'adaptador': 903,\n",
       " 'ska': 904,\n",
       " 'dancehall': 905,\n",
       " 'humeda': 906,\n",
       " 'recto': 907,\n",
       " 'fijacion': 908,\n",
       " 'napa': 909,\n",
       " 'bracken': 910,\n",
       " 'capa': 911,\n",
       " 'total': 912,\n",
       " 'white': 913,\n",
       " 'playstation': 914,\n",
       " 'tira': 915,\n",
       " 'plana': 916,\n",
       " '1tb': 917,\n",
       " 'guerra': 918,\n",
       " 'camping': 919,\n",
       " 'strass': 920,\n",
       " 'pedreria': 921,\n",
       " 'nautico': 922,\n",
       " 'box': 923,\n",
       " 'bateria': 924,\n",
       " 'brillo': 925,\n",
       " 'solar': 926,\n",
       " 'muerte': 927,\n",
       " 'suave': 928,\n",
       " 'mariposa': 929,\n",
       " 'claire': 930,\n",
       " 'sainte': 931,\n",
       " 'seguridad': 932,\n",
       " 'colchon': 933,\n",
       " 'pala': 934,\n",
       " 'day': 935,\n",
       " 'balance': 936,\n",
       " 'laser': 937,\n",
       " 'decorativo': 938,\n",
       " 'nido': 939,\n",
       " 'ver': 940,\n",
       " 'moon': 941,\n",
       " 'manopla': 942,\n",
       " 'clip': 943,\n",
       " 'piano': 944,\n",
       " 'aspirador': 945,\n",
       " 'plaza': 946,\n",
       " 'poster': 947,\n",
       " 'francesa': 948,\n",
       " 'i7': 949,\n",
       " 'toallita': 950,\n",
       " 'chancla': 951,\n",
       " 'mar': 952,\n",
       " 'man': 953,\n",
       " 'originals': 954,\n",
       " 'espejo': 955,\n",
       " 'hdr': 956,\n",
       " 'difusor': 957,\n",
       " 'roll': 958,\n",
       " 'banera': 959,\n",
       " 'episodio': 960,\n",
       " 'borla': 961,\n",
       " 'midi': 962,\n",
       " 'bandeja': 963,\n",
       " 'aplique': 964,\n",
       " 'bloc': 965,\n",
       " 'bordada': 966,\n",
       " 'skinny': 967,\n",
       " 'ebook': 968,\n",
       " 'colcha': 969,\n",
       " 'nueva': 970,\n",
       " 'fijador': 971,\n",
       " 'reparador': 972,\n",
       " 'vermeil': 973,\n",
       " 'brown': 974,\n",
       " 'liquido': 975,\n",
       " 'corrector': 976,\n",
       " 'helar': 977,\n",
       " 'cesta': 978,\n",
       " 'pink': 979,\n",
       " 'canale': 980,\n",
       " 'acondicionador': 981,\n",
       " 'didacticos': 982,\n",
       " 'trenzado': 983,\n",
       " 'ventana': 984,\n",
       " 'vidrio': 985,\n",
       " 'batidora': 986,\n",
       " 'ultimo': 987,\n",
       " 'charol': 988,\n",
       " 'cinco': 989,\n",
       " 'mercedita': 990,\n",
       " 'mum': 991,\n",
       " 'perfil': 992,\n",
       " 'block': 993,\n",
       " 'bag': 994,\n",
       " 'ipad': 995,\n",
       " 'ebooks': 996,\n",
       " 'basica': 997,\n",
       " 'brillante': 998,\n",
       " 'pagina': 999,\n",
       " 'amitie': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    1,    3,   94,\n",
       "       2461, 1432])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word_index has a unique ID assigned to each word in the data. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\t\tid\n",
      "--------------------\n",
      "ropa\t\t34\n",
      "deporte\t\t12\n",
      "abrigo\t\t94\n",
      "raqueta\t\t1645\n",
      "bebe\t\t4\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "test_string = \"ropa deporte abrigo raqueta bebe\"\n",
    "print(\"word\\t\\tid\")\n",
    "print(\"-\" * 20)\n",
    "for word in test_string.split():\n",
    "    print(\"%s\\t\\t%s\" % (word, word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\Enric\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "word_vectors = modelWV.wv\n",
    "vocabulary_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in modelWV:\n",
    "        embedding_matrix[i] = modelWV[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.rand(1, EMBEDDING_DIM)[0]\n",
    "            \n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "embedding_layer = Embedding(input_dim = vocabulary_size,\n",
    "                            output_dim = EMBEDDING_DIM,\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            name='w2v_embedding',\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWV.save(\"w2v_embedding_v1_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_2 = modelWV.wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = word_index\n",
    "#timesteps = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install python-pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GraphViz\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/e2/ef2581b5b86625657afd32030f90cf2717456c1d2b711ba074bf007c0f1a/graphviz-0.10.1-py2.py3-none-any.whl\n",
      "Installing collected packages: GraphViz\n",
      "Successfully installed GraphViz-0.10.1\n"
     ]
    }
   ],
   "source": [
    "! conda install python-graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204812, 24)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.vstack(data_vec)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.compile('rmsprop', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_embedding (Embedding)    (None, 24, 200)           9883800   \n",
      "=================================================================\n",
      "Total params: 9,883,800\n",
      "Trainable params: 0\n",
      "Non-trainable params: 9,883,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_i = Input(shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "encoded_h1 = Dense(128, activation='relu')(input_i)\n",
    "encoded_h2 = Dense(64, activation='relu')(encoded_h1)\n",
    "encoded_h3 = Dense(32, activation='relu')(encoded_h2)\n",
    "encoded_h4 = Dense(16, activation='relu')(encoded_h3)\n",
    "#encoded_h5 = Dense(8, activation='relu')(encoded_h4)\n",
    "\n",
    "latent = Dense(8, activation='relu', name = 'ENCODER')(encoded_h4)\n",
    "\n",
    "#decoder_h1 = Dense(8, activation='relu')(latent)\n",
    "decoder_h2 = Dense(16, activation='relu')(latent)\n",
    "decoder_h3 = Dense(32, activation='relu')(decoder_h2)\n",
    "decoder_h4 = Dense(64, activation='relu')(decoder_h3)\n",
    "decoder_h5 = Dense(128, activation='relu')(decoder_h4)\n",
    "\n",
    "output = Dense(EMBEDDING_DIM, activation='relu')(decoder_h5)\n",
    "\n",
    "autoencoder = Model(input_i,output)\n",
    "\n",
    "autoencoder.compile('rmsprop','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 24, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24, 128)           25728     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24, 16)            528       \n",
      "_________________________________________________________________\n",
      "ENCODER (Dense)              (None, 24, 8)             136       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24, 16)            144       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24, 32)            544       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24, 64)            2112      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24, 128)           8320      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 24, 200)           25800     \n",
      "=================================================================\n",
      "Total params: 73,648\n",
      "Trainable params: 73,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204812/204812 [==============================] - 15s 72us/step\n"
     ]
    }
   ],
   "source": [
    "X_embedded = model.predict(X_train, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "204812/204812 [==============================] - 132s 645us/step - loss: 0.5630\n",
      "Epoch 2/5\n",
      "204812/204812 [==============================] - 126s 616us/step - loss: 0.5614\n",
      "Epoch 3/5\n",
      " 15936/204812 [=>............................] - ETA: 2:06 - loss: 0.5592"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "autoencoder.fit(X_embedded,X_embedded,epochs=5,\n",
    "            batch_size=64, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('ENCODER').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder.save('encoder_text_V1.h5')\n",
    "#autoencoder = load_model('autoencoder_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1914\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1915\u001b[1;33m                 \u001b[0mworking_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1916\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    730\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1016\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] El sistema no puede encontrar el archivo especificado",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1921\u001b[0m                     prog=prog)\n\u001b[1;32m-> 1922\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1923\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-d239b4ac3c16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'encoder_plot.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         raise OSError(\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[1;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[1;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "plot_model(encoder, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[20000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.loc[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = X_embedded[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_embedded.copy()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "codes = encoder.predict(X_test)\n",
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_code = encoder.predict(query.reshape(1,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))\n",
    "query_code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "codes = codes.reshape(-1, 24*8)\n",
    "print(codes.shape)\n",
    "query_code = query_code.reshape(1, 24*8)\n",
    "print(query_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the KNN to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_neigh = 5\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neigh).fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(np.array(query_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_sent = X_test[indices]\n",
    "closest_sent = closest_sent.reshape(-1,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM); \n",
    "print(closest_sent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the closest text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                     8435465082995\n",
      "brand                                                     Pepe_Jeans\n",
      "text     moda accesorio cartera monedero mujer freida combinado flor\n",
      "Name: 24000, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                  8435465082971\n",
      "brand                                                  Pepe_Jeans\n",
      "text     moda accesorio cartera monedero mujer freida solapa flor\n",
      "Name: 23998, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                   8435465075928\n",
      "brand                                                   Pepe_Jeans\n",
      "text     moda accesorio cartera monedero mujer lica combinado azul\n",
      "Name: 24019, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                 1006219124705\n",
      "brand                                                       Tous\n",
      "text     moda accesorio cartera monedero mujer dorp pequeno rosa\n",
      "Name: 29602, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                 1006219123608\n",
      "brand                                                       Tous\n",
      "text     moda accesorio cartera monedero mujer dorp pequeno azul\n",
      "Name: 29601, dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mis_indices = indices.tolist()[0]\n",
    "for i in range(n_neigh):\n",
    "    print (data.loc[mis_indices[i]])\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
