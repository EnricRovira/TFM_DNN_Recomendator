{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.kdnuggets.com/wp-content/uploads/text-preprocessing-framework-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import contractions\n",
    "import inflect\n",
    "import re, unicodedata\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, \\\n",
    "                          UpSampling1D, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#import my_functions\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_ESP = list(set(stopwords.words('spanish')))\n",
    "STOPWORDS_CAT = open('stopwords_catalan.txt', 'r', encoding= 'UTF-8').read().split()\n",
    "STOPWORDS_ENG = list(set(stopwords.words('english')))\n",
    "CUSTOM  = ['uf', '100st', '100t', '100u', 'pal', 'zzb21601xu', 'x2']\n",
    "\n",
    "STOPWORDS_ALL = STOPWORDS_ESP + STOPWORDS_CAT + STOPWORDS_ENG + CUSTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file download with wget module\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-es.txt'  \n",
    "#wget.download(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = {}\n",
    "with open(\"lemmatization-es.txt\", encoding= 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            (key, val) = line.split()\n",
    "        except:\n",
    "            pass\n",
    "        lemmatizer[str(key)] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_inv = {v: k for k, v in lemmatizer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../02_Comprension de Datos/Descripciones204k_20190315.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo masculino con textura de mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                                          text  \n",
       "0                     Moda Mujer Abrigos Abrigo masculino con textura de mujer  \n",
       "1             Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono  \n",
       "2   Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés  \n",
       "3  Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos  \n",
       "4            Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una lista de marcas para poder quitarlas del texto en caso de que aparezcan, eliminamos las marcas duplicadas para comprimir el tamaño de la lista y optimizar el proceso de bsuqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                        1025942232486\n",
       "brand                                          Fox_Home\n",
       "text     Cine Series de Tv Futurama. 7ª Temporada (DVD)\n",
       "Name: 199000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[199000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "copia = data.copy()\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def normalize_brands(words):\n",
    "    words = str(words)\n",
    "    words  = words.strip().split()\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)    \n",
    "    words = list(dict.fromkeys(words)) #Remove duplicates\n",
    "    words = \" \".join(words)\n",
    "    return words\n",
    "    \n",
    "copia['brand'] = copia['brand'].apply(normalize_brands)\n",
    "brands = list(dict.fromkeys(copia['brand'].values))\n",
    "brands.remove('marvel')\n",
    "more_brands = ['tarocco', 'tucci', 'fjord', 'tommy', 'hilfiger', 'easy', 'wear', 'josma', 'jaipur', 'ralph', 'nemeziz']\n",
    "for brand in more_brands:\n",
    "    brands.append(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5091"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la normalización vamos a realizar una serie de tareas destinadas a poner todo el texto en un campo de juego nivelado: convertir todo el texto a la misma mayúscula o minúscula, eliminar la puntuación, convertir las cifras a sus equivalentes en palabras, etc. La normalización pone todas las palabras en pie de igualdad y permite que el procesamiento se realice de manera uniforme. Algunas de las técnicas que vamos a aplicar son:\n",
    "\n",
    "- Borrar caracteres extraños\n",
    "- Pasar todo el texto a minusculas\n",
    "- Quitar simbolos de puntuacion ( . , &, !, ?, ¿, /, etc)\n",
    "- Pasar numeros en digito a texto\n",
    "- Quitar stopwords\n",
    "- Stemming\n",
    "- Lexematizar\n",
    "\n",
    "**Importante:** Despues de la Nomralizacion trabajaremos a nivel de palabra o token en vez de a nivel de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ['corte ingles', 'bob esponja', 'color azul', 'cristiano ronaldo', 'fc barcelona', 'lionel messi', 'real madrid',\n",
    "         'azul oscuro', '1a temporada', '2a temporada', '3a temporada', '4a temporada', '5a temporada', '6a temporada',\n",
    "         '7a temporada', '8a temporada', 'mrs increible', 'mr increible', 'fc barcelona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        #new_word = unicodedata.normalize('NFD', word).encode('ascii', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if not word.isdigit():\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORDS_ALL and len(word) > 1:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def remove_brands(words):\n",
    "    \"\"\"Remove brands from the text\"\"\"\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        if word not in brands:\n",
    "            clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "def lemmatize(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        if word in lemmatizer_inv:\n",
    "            lemmas.append(lemmatizer_inv[word])\n",
    "        else: lemmas.append(word)\n",
    "    return lemmas\n",
    "\n",
    "def replace_ngrams(words):\n",
    "    for gram in ngrams:\n",
    "        if (gram in words):\n",
    "            g = \"_\".join(gram.split())\n",
    "            words = words.replace(gram, g)\n",
    "    return words\n",
    "\n",
    "def normalize(words):\n",
    "    #if pandas == True:\n",
    "    words  = words.strip().split()\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_brands(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = list(dict.fromkeys(words)) #Remove duplicates\n",
    "    words = \" \".join(words)\n",
    "    words = replace_ngrams(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#4min 4sec\n",
    "data_copy['text'] = data_copy['text'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo masculino textura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo doble faz cinturon tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo largo antelina woman corte_ingles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>moda mujer abrigo chaqueta termica efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>moda mujer abrigo parka algodon capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                     text  \n",
       "0                     moda mujer abrigo masculino textura  \n",
       "1               moda mujer abrigo doble faz cinturon tono  \n",
       "2     moda mujer abrigo largo antelina woman corte_ingles  \n",
       "3  moda mujer abrigo chaqueta termica efecto cortavientos  \n",
       "4                 moda mujer abrigo parka algodon capucha  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo masculino con textura de mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                                          text  \n",
       "0                     Moda Mujer Abrigos Abrigo masculino con textura de mujer  \n",
       "1             Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono  \n",
       "2   Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés  \n",
       "3  Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos  \n",
       "4            Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                        1025942232486\n",
      "brand                                          Fox_Home\n",
      "text     Cine Series de Tv Futurama. 7ª Temporada (DVD)\n",
      "Name: 199000, dtype: object\n",
      "id                                       1014970203072\n",
      "brand                                Polo_Ralph_Lauren\n",
      "text     moda mujer punto jersey lana polo cuello caja\n",
      "Name: 89000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#7000\n",
    "print(data.loc[199000])\n",
    "print(data_copy.loc[89000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204812,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max words in a sentence:          Mean words in a sentence:\n",
      "------------------------------------------------------------\n",
      "23                                8.059176220143351\n",
      "------------------------------------------------------------\n",
      "libro salud bienestar dieta nutricion general detox sen sano dentro bello clave nutricional rutina diarias eliminar toxina forma saludable energetica nutritiva tapa blanda\n"
     ]
    }
   ],
   "source": [
    "maxw = 0\n",
    "mean = 0\n",
    "max_s = 0\n",
    "for i, sentence in enumerate(data_copy['text']):\n",
    "    mean += len(sentence.split())\n",
    "    if maxw < len(sentence.split()):\n",
    "        maxw = len(sentence.split())\n",
    "        max_s = i\n",
    "print('Max words in a sentence:' + ' '*10 + 'Mean words in a sentence:')\n",
    "print(\"-\" * 60)\n",
    "print (f\"{maxw} {' '*30} {mean/len(data_copy['text'])}\")\n",
    "print(\"-\" * 60)\n",
    "print (data_copy['text'][max_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.to_csv('Texto_PreProcesado.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramas (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'moda', 'mujer', 'abrigo', 'masculino', 'textura', '1', 'doble', 'faz', 'cinturon']\n",
      "257064\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = nltk.word_tokenize(data_copy['text'].to_string())\n",
    "bag_of_words = list(dict.fromkeys(bag_of_words))\n",
    "print(bag_of_words[:10])\n",
    "print(len(bag_of_words)) \n",
    "#126k productos = 2_063_845 palabras\n",
    "#175k productos = 2_838_097 palabras\n",
    "#204k productos = 3_070_773 palabras -> 1_878_308 20/03/2019 -> no dups = 257042, ngrams =257063 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moda mujer abrigo masculino textura',\n",
       " 'moda mujer abrigo doble faz cinturon tono',\n",
       " 'moda mujer abrigo largo antelina woman corte_ingles',\n",
       " 'moda mujer abrigo chaqueta termica efecto cortavientos',\n",
       " 'moda mujer abrigo parka algodon capucha']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [sent for sent in data_copy['text']]\n",
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moda',\n",
       " 'mujer',\n",
       " 'abrigo',\n",
       " 'masculino',\n",
       " 'textura',\n",
       " 'moda mujer',\n",
       " 'mujer abrigo',\n",
       " 'abrigo masculino',\n",
       " 'masculino textura',\n",
       " 'doble']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.keys())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['moda', 'mujer', 'abrigo', 'masculino', 'textura'],\n",
       " ['moda', 'mujer', 'abrigo', 'doble', 'faz', 'cinturon', 'tono'],\n",
       " ['moda', 'mujer', 'abrigo', 'largo', 'antelina', 'woman', 'corte_ingles']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [word.split() for word in data_copy['text'].values]\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204812"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelWV = Word2Vec(sentences, workers = 3, min_count=5, window = 10, size = EMBEDDING_DIM)\n",
    "modelWV.train(sentences, total_examples=len(sentences), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWV.save(\"word2vec_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alpargata', 0.6294158697128296),\n",
       " ('botin', 0.6221684217453003),\n",
       " ('salon', 0.613247275352478),\n",
       " ('mocasin', 0.570500910282135),\n",
       " ('pepito', 0.5662637948989868),\n",
       " ('mercedita', 0.5502974987030029),\n",
       " ('nautico', 0.5464245676994324),\n",
       " ('chancla', 0.5452138781547546),\n",
       " ('bota', 0.5431485176086426),\n",
       " ('bailarinas', 0.5331730246543884)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Word2Vec.load(\"word2vec_model_v1\")\n",
    "wl = 'sandalia'\n",
    "modelWV.wv.most_similar (positive = wl)\n",
    "#model.wv.most_similar_cosmul(positive = ['disfraz', 'abrigo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carrycot', 0.47286760807037354),\n",
       " ('cambiador', 0.4512306749820709),\n",
       " ('trona', 0.44528281688690186),\n",
       " ('maternal', 0.44398361444473267),\n",
       " ('portabebes', 0.4420798122882843),\n",
       " ('cochesilla', 0.4380588233470917),\n",
       " ('gemelar', 0.42964616417884827),\n",
       " ('lux', 0.424551784992218),\n",
       " ('buggy', 0.42026326060295105),\n",
       " ('isofix', 0.4194536805152893)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similar_products(text):\n",
    "    text = normalize(text)\n",
    "    list_text = text.split()\n",
    "    most_similar = modelWV.wv.most_similar_cosmul(positive = list_text)\n",
    "    \n",
    "    return most_similar\n",
    "    \n",
    "similar_products('Silla de paseo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Sentences\n",
    "\n",
    "- Initialize tokenizer with num_words = MAX_NB_WORDS (200K). i.e. The tokenizer will perform a word count, sorted by number of occurences in descending order and pick top N words, 200K in this case \n",
    "- Use tokenizer's texts_to_sequences method to convert text to array of integers.\n",
    "- The arrays obtained from previous step might not be of uniform length, use pad_sequences method to obtain arrays with length equal to MAX_SEQUENCE_LENGTH (30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = len(bag_of_words) #200k\n",
    "MAX_SEQUENCE_LENGTH = 24                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = data_copy['text']\n",
    "all_text = all_text.drop_duplicates (keep = False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "data_sequences = tokenizer.texts_to_sequences(data_copy['text'])\n",
    "data_vec = pad_sequences(data_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49418 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#49421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moda': 1,\n",
       " 'accesorio': 2,\n",
       " 'mujer': 3,\n",
       " 'bebe': 4,\n",
       " 'infantil': 5,\n",
       " 'hombre': 6,\n",
       " 'cine': 7,\n",
       " 'hogar': 8,\n",
       " 'azul': 9,\n",
       " 'libro': 10,\n",
       " 'electronica': 11,\n",
       " 'deporte': 12,\n",
       " 'musica': 13,\n",
       " 'nina': 14,\n",
       " 'tapa': 15,\n",
       " 'dvd': 16,\n",
       " 'negro': 17,\n",
       " 'corte': 18,\n",
       " 'juguete': 19,\n",
       " 'nino': 20,\n",
       " 'ingles': 21,\n",
       " 'perfumeria': 22,\n",
       " 'mes': 23,\n",
       " 'color': 24,\n",
       " 'regalo': 25,\n",
       " 'ano': 26,\n",
       " 'cd': 27,\n",
       " 'zapato': 28,\n",
       " 'original': 29,\n",
       " 'blanda': 30,\n",
       " 'piel': 31,\n",
       " 'juego': 32,\n",
       " 'gris': 33,\n",
       " 'ropa': 34,\n",
       " 'papeleria': 35,\n",
       " 'rosa': 36,\n",
       " 'menaje': 37,\n",
       " 'electrodomestico': 38,\n",
       " 'blanco': 39,\n",
       " 'bolso': 40,\n",
       " 'cocina': 41,\n",
       " 'estampado': 42,\n",
       " 'mesa': 43,\n",
       " 'camiseta': 44,\n",
       " 'silla': 45,\n",
       " 'novela': 46,\n",
       " 'zapatilla': 47,\n",
       " 'bluray': 48,\n",
       " 'pantalon': 49,\n",
       " 'paseo': 50,\n",
       " 'bano': 51,\n",
       " 'pack': 52,\n",
       " 'rostro': 53,\n",
       " 'coche': 54,\n",
       " 'bisuteria': 55,\n",
       " 'decoracion': 56,\n",
       " 'reloj': 57,\n",
       " 'jersey': 58,\n",
       " 'chaqueta': 59,\n",
       " 'punto': 60,\n",
       " 'rojo': 61,\n",
       " 'camisa': 62,\n",
       " 'manga': 63,\n",
       " 'marino': 64,\n",
       " 'marron': 65,\n",
       " 'cuidado': 66,\n",
       " 'general': 67,\n",
       " 'funda': 68,\n",
       " 'joyeria': 69,\n",
       " 'verde': 70,\n",
       " 'textil': 71,\n",
       " 'mini': 72,\n",
       " 'vestido': 73,\n",
       " 'polo': 74,\n",
       " 'deportiva': 75,\n",
       " 'maquillaje': 76,\n",
       " 'mochila': 77,\n",
       " 'videojuego': 78,\n",
       " 'parafarmacia': 79,\n",
       " 'plata': 80,\n",
       " 'raya': 81,\n",
       " 'mascota': 82,\n",
       " 'negra': 83,\n",
       " 'lenceria': 84,\n",
       " 'figura': 85,\n",
       " 'grande': 86,\n",
       " 'clasico': 87,\n",
       " 'dura': 88,\n",
       " 'algodon': 89,\n",
       " 'cama': 90,\n",
       " 'multicolor': 91,\n",
       " 'tratamiento': 92,\n",
       " 'beige': 93,\n",
       " 'abrigo': 94,\n",
       " 'cartera': 95,\n",
       " 'movil': 96,\n",
       " 'bandolera': 97,\n",
       " 'comic': 98,\n",
       " 'complemento': 99,\n",
       " 'cuerpo': 100,\n",
       " 'tuc': 101,\n",
       " 'carro': 102,\n",
       " 'bota': 103,\n",
       " 'pulsera': 104,\n",
       " 'mililitro': 105,\n",
       " 'crema': 106,\n",
       " 'cremallera': 107,\n",
       " 'star': 108,\n",
       " 'baby': 109,\n",
       " 'cotton': 110,\n",
       " 'hidratante': 111,\n",
       " 'mueble': 112,\n",
       " 'blusa': 113,\n",
       " 'peluche': 114,\n",
       " 'top': 115,\n",
       " 'set': 116,\n",
       " 'sonido': 117,\n",
       " 'sol': 118,\n",
       " 'sudadera': 119,\n",
       " 'equipaje': 120,\n",
       " 'lpvinilo': 121,\n",
       " 'capacidad': 122,\n",
       " 'gafas': 123,\n",
       " 'pop': 124,\n",
       " 'cuello': 125,\n",
       " 'flor': 126,\n",
       " 'escritura': 127,\n",
       " 'portatil': 128,\n",
       " 'espanola': 129,\n",
       " 'texto': 130,\n",
       " 'neceser': 131,\n",
       " 'espanol': 132,\n",
       " 'muneca': 133,\n",
       " 'muneco': 134,\n",
       " 'pieza': 135,\n",
       " 'serie': 136,\n",
       " 'rock': 137,\n",
       " 'home': 138,\n",
       " 'deportivas': 139,\n",
       " 'gorro': 140,\n",
       " 'vehiculo': 141,\n",
       " 'placa': 142,\n",
       " 'acero': 143,\n",
       " 'auto': 144,\n",
       " 'ojo': 145,\n",
       " 'pendiente': 146,\n",
       " 'monedero': 147,\n",
       " 'campana': 148,\n",
       " 'cuna': 149,\n",
       " 'cuadro': 150,\n",
       " 'new': 151,\n",
       " 'oro': 152,\n",
       " 'tv': 153,\n",
       " 'horno': 154,\n",
       " 'detalle': 155,\n",
       " 'pijama': 156,\n",
       " 'fiesta': 157,\n",
       " 'bolsa': 158,\n",
       " 'futbol': 159,\n",
       " 'jardin': 160,\n",
       " 'cordon': 161,\n",
       " 'corporal': 162,\n",
       " 'corta': 163,\n",
       " 'articulado': 164,\n",
       " 'preescolar': 165,\n",
       " 'juice': 166,\n",
       " 'manir': 167,\n",
       " 'escritorio': 168,\n",
       " 'maleta': 169,\n",
       " 'wars': 170,\n",
       " 'aventura': 171,\n",
       " 'pelo': 172,\n",
       " 'narrativa': 173,\n",
       " 'unisex': 174,\n",
       " 'western': 175,\n",
       " 'perro': 176,\n",
       " 'metal': 177,\n",
       " 'grupo': 178,\n",
       " 'recambio': 179,\n",
       " 'hombro': 180,\n",
       " 'calcetin': 181,\n",
       " 'juvenil': 182,\n",
       " 'larga': 183,\n",
       " 'playa': 184,\n",
       " 'lectura': 185,\n",
       " 'bordado': 186,\n",
       " 'higiene': 187,\n",
       " 'vaquero': 188,\n",
       " 'playsets': 189,\n",
       " 'preparacion': 190,\n",
       " 'natural': 191,\n",
       " 'mundo': 192,\n",
       " 'internacional': 193,\n",
       " 'cancion': 194,\n",
       " 'one': 195,\n",
       " 'caja': 196,\n",
       " 'pro': 197,\n",
       " 'banda': 198,\n",
       " 'black': 199,\n",
       " 'casual': 200,\n",
       " 'estuche': 201,\n",
       " 'cuaderno': 202,\n",
       " 'ps4': 203,\n",
       " 'viaje': 204,\n",
       " 'alimento': 205,\n",
       " 'doble': 206,\n",
       " 'frigorifico': 207,\n",
       " 'congelador': 208,\n",
       " 'ciencia': 209,\n",
       " 'plus': 210,\n",
       " 'dia': 211,\n",
       " 'facial': 212,\n",
       " 'amarillo': 213,\n",
       " 'entretenimiento': 214,\n",
       " 'capucha': 215,\n",
       " 'falda': 216,\n",
       " 'musical': 217,\n",
       " 'salud': 218,\n",
       " 'classic': 219,\n",
       " 'libre': 220,\n",
       " 'gel': 221,\n",
       " 'accion': 222,\n",
       " 'emidio': 223,\n",
       " 'bienestar': 224,\n",
       " 'gato': 225,\n",
       " 'panuelo': 226,\n",
       " 'plato': 227,\n",
       " 'colgante': 228,\n",
       " 'corto': 229,\n",
       " 'informatica': 230,\n",
       " 'taza': 231,\n",
       " 'fular': 232,\n",
       " 'sombrero': 233,\n",
       " 'antiedad': 234,\n",
       " 'drama': 235,\n",
       " 'dieta': 236,\n",
       " 'bolsillo': 237,\n",
       " 'carcasa': 238,\n",
       " 'autoayuda': 239,\n",
       " 'print': 240,\n",
       " 'blanca': 241,\n",
       " 'joven': 242,\n",
       " 'botin': 243,\n",
       " 'age': 244,\n",
       " 'boton': 245,\n",
       " 'agua': 246,\n",
       " 'oscuro': 247,\n",
       " 'pc': 248,\n",
       " 'talla': 249,\n",
       " 'pequena': 250,\n",
       " 'agenda': 251,\n",
       " 'fragancia': 252,\n",
       " 'romantica': 253,\n",
       " 'capazo': 254,\n",
       " 'joe': 255,\n",
       " 'confort': 256,\n",
       " 'saco': 257,\n",
       " 'bluetooth': 258,\n",
       " 'protector': 259,\n",
       " 'historica': 260,\n",
       " 'calendario': 261,\n",
       " 'auricular': 262,\n",
       " 'body': 263,\n",
       " 'pelicula': 264,\n",
       " 'bicolor': 265,\n",
       " 'vela': 266,\n",
       " 'primera': 267,\n",
       " 'asa': 268,\n",
       " 'eau': 269,\n",
       " 'cierre': 270,\n",
       " 'formula': 271,\n",
       " 'montana': 272,\n",
       " 'regular': 273,\n",
       " 'frontal': 274,\n",
       " 'slim': 275,\n",
       " 'volante': 276,\n",
       " 'terror': 277,\n",
       " 'lujo': 278,\n",
       " 'collar': 279,\n",
       " 'temporada': 280,\n",
       " 'interior': 281,\n",
       " 'policiaca': 282,\n",
       " 'documental': 283,\n",
       " 'milimetro': 284,\n",
       " 'topo': 285,\n",
       " 'regional': 286,\n",
       " 'cazadora': 287,\n",
       " 'recomendadas': 288,\n",
       " 'sandalia': 289,\n",
       " 'cinturon': 290,\n",
       " 'dorado': 291,\n",
       " 'comedia': 292,\n",
       " 'intriga': 293,\n",
       " 'suspense': 294,\n",
       " 'estrella': 295,\n",
       " 'fantastico': 296,\n",
       " 'modelo': 297,\n",
       " 'liso': 298,\n",
       " 'do': 299,\n",
       " 'aroma': 300,\n",
       " 'impresora': 301,\n",
       " 'largo': 302,\n",
       " 'ordenador': 303,\n",
       " 'sabana': 304,\n",
       " 'rigida': 305,\n",
       " 'limpieza': 306,\n",
       " 'sonoras': 307,\n",
       " 'vaso': 308,\n",
       " 'poprock': 309,\n",
       " 'jeans': 310,\n",
       " 'ficcion': 311,\n",
       " 'leggins': 312,\n",
       " 'pan': 313,\n",
       " 'smartphones': 314,\n",
       " 'hop': 315,\n",
       " 'jazz': 316,\n",
       " 'infancia': 317,\n",
       " 'naranja': 318,\n",
       " 'conjunto': 319,\n",
       " 'libreta': 320,\n",
       " 'animacion': 321,\n",
       " 'braga': 322,\n",
       " 'dance': 323,\n",
       " 'puzzle': 324,\n",
       " 'soulfunkrb': 325,\n",
       " 'jazzblues': 326,\n",
       " 'piccolo': 327,\n",
       " 'disfraz': 328,\n",
       " 'duo': 329,\n",
       " 'tutto': 330,\n",
       " 'multifuncion': 331,\n",
       " 'ordenacion': 332,\n",
       " 'chocolate': 333,\n",
       " 'cabello': 334,\n",
       " 'neck': 335,\n",
       " 'led': 336,\n",
       " 'correccion': 337,\n",
       " 'estampada': 338,\n",
       " 'encaje': 339,\n",
       " 'claro': 340,\n",
       " 'anime': 341,\n",
       " 'floral': 342,\n",
       " 'lisa': 343,\n",
       " 'logo': 344,\n",
       " 'running': 345,\n",
       " 'romantico': 346,\n",
       " 'corbata': 347,\n",
       " 'tinta': 348,\n",
       " 'hip': 349,\n",
       " 'hard': 350,\n",
       " 'bermuda': 351,\n",
       " 'construccion': 352,\n",
       " 'rap': 353,\n",
       " 'edicion': 354,\n",
       " 'green': 355,\n",
       " 'rinonera': 356,\n",
       " 'cojin': 357,\n",
       " 'frikis': 358,\n",
       " 'transparente': 359,\n",
       " 'aire': 360,\n",
       " 'bermudas': 361,\n",
       " 'altavoz': 362,\n",
       " 'blazer': 363,\n",
       " 'esqui': 364,\n",
       " 'peto': 365,\n",
       " 'mediana': 366,\n",
       " 'puerta': 367,\n",
       " 'minicunas': 368,\n",
       " 'educativo': 369,\n",
       " 'house': 370,\n",
       " 'pequeno': 371,\n",
       " 'cafe': 372,\n",
       " 'collection': 373,\n",
       " 'ferreteria': 374,\n",
       " 'klein': 375,\n",
       " 'padel': 376,\n",
       " 'calvin': 377,\n",
       " 'short': 378,\n",
       " 'noche': 379,\n",
       " 'tecnologia': 380,\n",
       " 'ranita': 381,\n",
       " 'belico': 382,\n",
       " 'superheroes': 383,\n",
       " 'anillo': 384,\n",
       " 'roja': 385,\n",
       " 'pepe': 386,\n",
       " 'mejor': 387,\n",
       " 'perla': 388,\n",
       " 'kg': 389,\n",
       " 'casa': 390,\n",
       " 'gorra': 391,\n",
       " 'anual': 392,\n",
       " 'capilar': 393,\n",
       " 'antiarruga': 394,\n",
       " 'madrid': 395,\n",
       " 'hd': 396,\n",
       " 'invierno': 397,\n",
       " 'redondo': 398,\n",
       " 'digital': 399,\n",
       " 'snow': 400,\n",
       " 'hispanoamericana': 401,\n",
       " 'media': 402,\n",
       " 'coleccion': 403,\n",
       " 'corazon': 404,\n",
       " 'bicicleta': 405,\n",
       " 'hoja': 406,\n",
       " 'pantalla': 407,\n",
       " 'iphone': 408,\n",
       " 'iluminacion': 409,\n",
       " 'salon': 410,\n",
       " 'vida': 411,\n",
       " 'copa': 412,\n",
       " 'super': 413,\n",
       " 'granate': 414,\n",
       " 'ortiz': 415,\n",
       " 'boligrafo': 416,\n",
       " 'coast': 417,\n",
       " 'papel': 418,\n",
       " 'solapa': 419,\n",
       " 'te': 420,\n",
       " 'fitness': 421,\n",
       " 'real': 422,\n",
       " 'sujetador': 423,\n",
       " 'comida': 424,\n",
       " 'madera': 425,\n",
       " 'paris': 426,\n",
       " 'calzado': 427,\n",
       " 'shopping': 428,\n",
       " 'xbox': 429,\n",
       " 'ensamblaje': 430,\n",
       " 'compositor': 431,\n",
       " 'tipo': 432,\n",
       " 'minicuna': 433,\n",
       " 'animal': 434,\n",
       " 'primaria': 435,\n",
       " 'tapizado': 436,\n",
       " 'panties': 437,\n",
       " 'instrumento': 438,\n",
       " 'ii': 439,\n",
       " 'luz': 440,\n",
       " 'kit': 441,\n",
       " 'cambiador': 442,\n",
       " 'banador': 443,\n",
       " 'blue': 444,\n",
       " 'celeste': 445,\n",
       " 'adulto': 446,\n",
       " 'extranjera': 447,\n",
       " '3d': 448,\n",
       " 'modulares': 449,\n",
       " 'artistico': 450,\n",
       " 'exterior': 451,\n",
       " 'venir': 452,\n",
       " 'contraste': 453,\n",
       " 'lana': 454,\n",
       " 'disco': 455,\n",
       " 'wonderful': 456,\n",
       " 'radiocontrol': 457,\n",
       " 'aro': 458,\n",
       " 'love': 459,\n",
       " 'reversible': 460,\n",
       " 'cabina': 461,\n",
       " 'acuatico': 462,\n",
       " 'north': 463,\n",
       " 'galaxy': 464,\n",
       " 'harry': 465,\n",
       " 'edition': 466,\n",
       " 'parka': 467,\n",
       " 'vacuna': 468,\n",
       " 'preparar': 469,\n",
       " 'escote': 470,\n",
       " 'toalla': 471,\n",
       " 'ligeras': 472,\n",
       " 'deportivo': 473,\n",
       " 'silicona': 474,\n",
       " 'especial': 475,\n",
       " 'usb': 476,\n",
       " 'control': 477,\n",
       " 'habilidad': 478,\n",
       " 'labio': 479,\n",
       " 'face': 480,\n",
       " 'burdeos': 481,\n",
       " 'camel': 482,\n",
       " 'capota': 483,\n",
       " 'correa': 484,\n",
       " 'urban': 485,\n",
       " 'circuito': 486,\n",
       " 'potter': 487,\n",
       " 'tres': 488,\n",
       " 'switch': 489,\n",
       " 'adorno': 490,\n",
       " 'memoria': 491,\n",
       " 'piscina': 492,\n",
       " 'barra': 493,\n",
       " 'aceite': 494,\n",
       " 'bluchers': 495,\n",
       " 'polipiel': 496,\n",
       " 'monitor': 497,\n",
       " 'manta': 498,\n",
       " 'efecto': 499,\n",
       " 'lateral': 500,\n",
       " 'mayor': 501,\n",
       " 'champus': 502,\n",
       " 'champu': 503,\n",
       " 'equipaciones': 504,\n",
       " 'smart': 505,\n",
       " 'pie': 506,\n",
       " 'rectangular': 507,\n",
       " 'bufanda': 508,\n",
       " 'latino': 509,\n",
       " 'carga': 510,\n",
       " 'gaming': 511,\n",
       " 'mocasin': 512,\n",
       " 'ssd': 513,\n",
       " 'lavadora': 514,\n",
       " 'ultra': 515,\n",
       " 'diadema': 516,\n",
       " 'vista': 517,\n",
       " 'decorativas': 518,\n",
       " 'full': 519,\n",
       " 'terraza': 520,\n",
       " 'sinfonico': 521,\n",
       " 'city': 522,\n",
       " 'country': 523,\n",
       " 'cartucho': 524,\n",
       " 'trona': 525,\n",
       " 'emporio': 526,\n",
       " 'cafetera': 527,\n",
       " 'dependencia': 528,\n",
       " '1a': 529,\n",
       " 'sombra': 530,\n",
       " 'ortopedia': 531,\n",
       " 'tarjeta': 532,\n",
       " 'fotografia': 533,\n",
       " 'objetivo': 534,\n",
       " 'pico': 535,\n",
       " 'decorativa': 536,\n",
       " 'techno': 537,\n",
       " 'woman': 538,\n",
       " 'ducha': 539,\n",
       " 'mascara': 540,\n",
       " 'r': 541,\n",
       " 'p': 542,\n",
       " 'm': 543,\n",
       " 'kaki': 544,\n",
       " 'llavero': 545,\n",
       " '4k': 546,\n",
       " 'toilette': 547,\n",
       " 'air': 548,\n",
       " 'isize': 549,\n",
       " 'nylon': 550,\n",
       " 'bailarinas': 551,\n",
       " 'morado': 552,\n",
       " 'teenstore': 553,\n",
       " 'jugar': 554,\n",
       " 'tarjetero': 555,\n",
       " 'coral': 556,\n",
       " 'duro': 557,\n",
       " 'locion': 558,\n",
       " 'ciclismo': 559,\n",
       " 'premium': 560,\n",
       " 'velcro': 561,\n",
       " 'aluminio': 562,\n",
       " 'combi': 563,\n",
       " 'little': 564,\n",
       " 'redonda': 565,\n",
       " 'pen': 566,\n",
       " 'chasis': 567,\n",
       " 'manual': 568,\n",
       " 'fucsia': 569,\n",
       " 'roller': 570,\n",
       " 'sport': 571,\n",
       " 'grey': 572,\n",
       " 'marvel': 573,\n",
       " 'sephora': 574,\n",
       " 'vertical': 575,\n",
       " 'malla': 576,\n",
       " 'wifi': 577,\n",
       " 'pasito': 578,\n",
       " 'encuadernadas': 579,\n",
       " 'diseno': 580,\n",
       " 'cinta': 581,\n",
       " 'chino': 582,\n",
       " 'lampara': 583,\n",
       " 'roberto': 584,\n",
       " 'dorada': 585,\n",
       " 'chandals': 586,\n",
       " 'no': 587,\n",
       " 'tono': 588,\n",
       " 'drive': 589,\n",
       " 'pluma': 590,\n",
       " 'mascarilla': 591,\n",
       " 'tejido': 592,\n",
       " 'guante': 593,\n",
       " 'equipacion': 594,\n",
       " 'botella': 595,\n",
       " 'alto': 596,\n",
       " 'sobreponer': 597,\n",
       " 'piedra': 598,\n",
       " 'goma': 599,\n",
       " 'americana': 600,\n",
       " 'cadena': 601,\n",
       " 'luna': 602,\n",
       " 'portadocumentos': 603,\n",
       " 'mantel': 604,\n",
       " 'plateado': 605,\n",
       " 'parfum': 606,\n",
       " 'mano': 607,\n",
       " 'leotardo': 608,\n",
       " 'historia': 609,\n",
       " 'southern': 610,\n",
       " 'serraje': 611,\n",
       " 'plegable': 612,\n",
       " 'bluegrass': 613,\n",
       " 'texmex': 614,\n",
       " 'vaquera': 615,\n",
       " 'lente': 616,\n",
       " 'tierra': 617,\n",
       " 'studio': 618,\n",
       " 'cataluna': 619,\n",
       " 'ajustable': 620,\n",
       " 'cf': 621,\n",
       " 'espiral': 622,\n",
       " 'turquesa': 623,\n",
       " 'tirante': 624,\n",
       " 'traje': 625,\n",
       " 'bso': 626,\n",
       " 'lino': 627,\n",
       " 'maletin': 628,\n",
       " 'extractoras': 629,\n",
       " 'bajo': 630,\n",
       " 'molly': 631,\n",
       " 'lunar': 632,\n",
       " 'tote': 633,\n",
       " 'montura': 634,\n",
       " 'dc': 635,\n",
       " 'dragon': 636,\n",
       " 'colchoneta': 637,\n",
       " 'delantero': 638,\n",
       " 'tricot': 639,\n",
       " 'tenis': 640,\n",
       " 'vol': 641,\n",
       " 'lapiz': 642,\n",
       " 'nordica': 643,\n",
       " 'a4': 644,\n",
       " 'plumifero': 645,\n",
       " 'raton': 646,\n",
       " 'petit': 647,\n",
       " 'eye': 648,\n",
       " 'slippers': 649,\n",
       " 'robot': 650,\n",
       " 'electronico': 651,\n",
       " 'boch': 652,\n",
       " 'postre': 653,\n",
       " 'verino': 654,\n",
       " 'panty': 655,\n",
       " 'amor': 656,\n",
       " 'villeroy': 657,\n",
       " 'life': 658,\n",
       " '2a': 659,\n",
       " 'fantasia': 660,\n",
       " 'go': 661,\n",
       " 'marca': 662,\n",
       " 'paraguas': 663,\n",
       " 'cuero': 664,\n",
       " 'pelele': 665,\n",
       " 'auxiliares': 666,\n",
       " 'pique': 667,\n",
       " 'basic': 668,\n",
       " 'oficial': 669,\n",
       " 'induccion': 670,\n",
       " 'rueda': 671,\n",
       " 'semana': 672,\n",
       " 'padre': 673,\n",
       " 'bass': 674,\n",
       " 'zona': 675,\n",
       " 'desodorante': 676,\n",
       " 'gastronomia': 677,\n",
       " 'mono': 678,\n",
       " 'exclusivo': 679,\n",
       " 'bikini': 680,\n",
       " 'circonitas': 681,\n",
       " 'camara': 682,\n",
       " 'room': 683,\n",
       " 'adolfo': 684,\n",
       " 'dominguez': 685,\n",
       " 'pitillo': 686,\n",
       " 'reacondicionados': 687,\n",
       " 'princesa': 688,\n",
       " 'lona': 689,\n",
       " 'cristal': 690,\n",
       " 'light': 691,\n",
       " 'barcelona': 692,\n",
       " 'alfombra': 693,\n",
       " 'contorno': 694,\n",
       " 'mickey': 695,\n",
       " 'hebilla': 696,\n",
       " 'elastico': 697,\n",
       " 'pinza': 698,\n",
       " 'linea': 699,\n",
       " 'mediano': 700,\n",
       " 'reacondicionado': 701,\n",
       " 'grabado': 702,\n",
       " 'golf': 703,\n",
       " 'autor': 704,\n",
       " 'oso': 705,\n",
       " 'silver': 706,\n",
       " 'bateau': 707,\n",
       " 'yves': 708,\n",
       " 'loreal': 709,\n",
       " 'estilo': 710,\n",
       " 'convertible': 711,\n",
       " 'compartimento': 712,\n",
       " 'rodio': 713,\n",
       " 'pompon': 714,\n",
       " 'tela': 715,\n",
       " 'jacquard': 716,\n",
       " 'material': 717,\n",
       " 'gold': 718,\n",
       " 'deluxe': 719,\n",
       " 'fc': 720,\n",
       " 'gran': 721,\n",
       " 'crudo': 722,\n",
       " 'acetato': 723,\n",
       " 'combinado': 724,\n",
       " 'grafica': 725,\n",
       " 'pared': 726,\n",
       " 'solares': 727,\n",
       " 'acolchada': 728,\n",
       " 'mate': 729,\n",
       " 'spray': 730,\n",
       " 'comedor': 731,\n",
       " 'limpiadoras': 732,\n",
       " 'chill': 733,\n",
       " 'nude': 734,\n",
       " 'archivo': 735,\n",
       " 'nordicas': 736,\n",
       " 'recibidor': 737,\n",
       " 'multilingue': 738,\n",
       " 'proteccion': 739,\n",
       " 'sofa': 740,\n",
       " 'spf': 741,\n",
       " 'portatiles': 742,\n",
       " 'cepillo': 743,\n",
       " 'precio': 744,\n",
       " 'jabon': 745,\n",
       " 'pestana': 746,\n",
       " 'saint': 747,\n",
       " 'look': 748,\n",
       " 'jack': 749,\n",
       " 'mostaza': 750,\n",
       " 'gr': 751,\n",
       " 'serum': 752,\n",
       " 'lentejuela': 753,\n",
       " 'michael': 754,\n",
       " 'kors': 755,\n",
       " 'nutricion': 756,\n",
       " 'vinilo': 757,\n",
       " 'junior': 758,\n",
       " 'comunion': 759,\n",
       " 'inalambrico': 760,\n",
       " 'pintado': 761,\n",
       " 'visual': 762,\n",
       " 'colecho': 763,\n",
       " 'iii': 764,\n",
       " 'ambient': 765,\n",
       " 'tacon': 766,\n",
       " 'live': 767,\n",
       " 'aspiracion': 768,\n",
       " 'cover': 769,\n",
       " 'power': 770,\n",
       " 'club': 771,\n",
       " 'verano': 772,\n",
       " 'soporte': 773,\n",
       " 'termo': 774,\n",
       " 'rotulador': 775,\n",
       " 'happy': 776,\n",
       " 'convencional': 777,\n",
       " 'men': 778,\n",
       " 'pure': 779,\n",
       " 'lavavajillas': 780,\n",
       " '8gb': 781,\n",
       " 'practica': 782,\n",
       " 'extraible': 783,\n",
       " 'litro': 784,\n",
       " 'jugador': 785,\n",
       " 'dibujo': 786,\n",
       " 'electrico': 787,\n",
       " 'reposteria': 788,\n",
       " 'classics': 789,\n",
       " 'coco': 790,\n",
       " 'enviar': 791,\n",
       " 'moises': 792,\n",
       " 'cintura': 793,\n",
       " 'punta': 794,\n",
       " 'vajilla': 795,\n",
       " 'completo': 796,\n",
       " 'partir': 797,\n",
       " 'lite': 798,\n",
       " 'bella': 799,\n",
       " 'kids': 800,\n",
       " 'extensible': 801,\n",
       " 'almohada': 802,\n",
       " 'estrenar': 803,\n",
       " 'cuchillo': 804,\n",
       " 'sillon': 805,\n",
       " 'cortina': 806,\n",
       " 'metalico': 807,\n",
       " 'fashion': 808,\n",
       " 'tablet': 809,\n",
       " 'leche': 810,\n",
       " 'base': 811,\n",
       " 'vestir': 812,\n",
       " 'oficina': 813,\n",
       " 'lila': 814,\n",
       " 'pena': 815,\n",
       " 'porta': 816,\n",
       " 'radio': 817,\n",
       " 'blister': 818,\n",
       " 'britax': 819,\n",
       " 'romer': 820,\n",
       " 'balsamo': 821,\n",
       " 'individual': 822,\n",
       " 'microfono': 823,\n",
       " 'reflex': 824,\n",
       " 'una': 825,\n",
       " 'geometrico': 826,\n",
       " 'embarazo': 827,\n",
       " 'clasificacion': 828,\n",
       " 'basico': 829,\n",
       " 'alta': 830,\n",
       " 'fleco': 831,\n",
       " 'book': 832,\n",
       " 'tb': 833,\n",
       " 'chaleco': 834,\n",
       " 'pasta': 835,\n",
       " 'primero': 836,\n",
       " 'max': 837,\n",
       " 'split': 838,\n",
       " 'plancha': 839,\n",
       " 'agatha': 840,\n",
       " 'hdd': 841,\n",
       " 'cable': 842,\n",
       " 'circonita': 843,\n",
       " 'acolchado': 844,\n",
       " 'nafnaf': 845,\n",
       " 'hobo': 846,\n",
       " 'ball': 847,\n",
       " 'brocha': 848,\n",
       " 'reggae': 849,\n",
       " 'hilo': 850,\n",
       " 'run': 851,\n",
       " 'walking': 852,\n",
       " 'polvo': 853,\n",
       " 'parte': 854,\n",
       " 'vapor': 855,\n",
       " 'charm': 856,\n",
       " 'inverter': 857,\n",
       " 'popular': 858,\n",
       " 'guitarra': 859,\n",
       " 'nacido': 860,\n",
       " '3a': 861,\n",
       " 'fina': 862,\n",
       " 'broche': 863,\n",
       " 'royal': 864,\n",
       " 'equipo': 865,\n",
       " 'exchange': 866,\n",
       " 'pana': 867,\n",
       " 'ingle': 868,\n",
       " 'compacto': 869,\n",
       " 'world': 870,\n",
       " 'pensar': 871,\n",
       " 'evil': 872,\n",
       " 'melange': 873,\n",
       " 'music': 874,\n",
       " 'carpeta': 875,\n",
       " 'fluido': 876,\n",
       " 'cat': 877,\n",
       " 'mando': 878,\n",
       " 'acondicionado': 879,\n",
       " 'literatura': 880,\n",
       " 'cuadrada': 881,\n",
       " 'vidal': 882,\n",
       " 'templado': 883,\n",
       " 'frigh': 884,\n",
       " 'secreto': 885,\n",
       " 'capsula': 886,\n",
       " 'forma': 887,\n",
       " 'sobremesa': 888,\n",
       " 'skin': 889,\n",
       " 'cajon': 890,\n",
       " 'lifting': 891,\n",
       " 'dub': 892,\n",
       " 'automatico': 893,\n",
       " 'recien': 894,\n",
       " 'armour': 895,\n",
       " 'tacha': 896,\n",
       " 'lurex': 897,\n",
       " 'fred': 898,\n",
       " 'tricolor': 899,\n",
       " 'limitada': 900,\n",
       " 'varilla': 901,\n",
       " 'pili': 902,\n",
       " 'adaptador': 903,\n",
       " 'ska': 904,\n",
       " 'dancehall': 905,\n",
       " 'humeda': 906,\n",
       " 'recto': 907,\n",
       " 'fijacion': 908,\n",
       " 'napa': 909,\n",
       " 'bracken': 910,\n",
       " 'capa': 911,\n",
       " 'total': 912,\n",
       " 'white': 913,\n",
       " 'playstation': 914,\n",
       " 'tira': 915,\n",
       " 'plana': 916,\n",
       " '1tb': 917,\n",
       " 'guerra': 918,\n",
       " 'camping': 919,\n",
       " 'strass': 920,\n",
       " 'pedreria': 921,\n",
       " 'nautico': 922,\n",
       " 'box': 923,\n",
       " 'bateria': 924,\n",
       " 'brillo': 925,\n",
       " 'solar': 926,\n",
       " 'muerte': 927,\n",
       " 'suave': 928,\n",
       " 'mariposa': 929,\n",
       " 'claire': 930,\n",
       " 'sainte': 931,\n",
       " 'seguridad': 932,\n",
       " 'colchon': 933,\n",
       " 'pala': 934,\n",
       " 'day': 935,\n",
       " 'balance': 936,\n",
       " 'laser': 937,\n",
       " 'decorativo': 938,\n",
       " 'nido': 939,\n",
       " 'ver': 940,\n",
       " 'moon': 941,\n",
       " 'manopla': 942,\n",
       " 'clip': 943,\n",
       " 'piano': 944,\n",
       " 'aspirador': 945,\n",
       " 'plaza': 946,\n",
       " 'poster': 947,\n",
       " 'francesa': 948,\n",
       " 'i7': 949,\n",
       " 'toallita': 950,\n",
       " 'chancla': 951,\n",
       " 'mar': 952,\n",
       " 'man': 953,\n",
       " 'originals': 954,\n",
       " 'espejo': 955,\n",
       " 'hdr': 956,\n",
       " 'difusor': 957,\n",
       " 'roll': 958,\n",
       " 'banera': 959,\n",
       " 'episodio': 960,\n",
       " 'borla': 961,\n",
       " 'midi': 962,\n",
       " 'bandeja': 963,\n",
       " 'aplique': 964,\n",
       " 'bloc': 965,\n",
       " 'bordada': 966,\n",
       " 'skinny': 967,\n",
       " 'ebook': 968,\n",
       " 'colcha': 969,\n",
       " 'nueva': 970,\n",
       " 'fijador': 971,\n",
       " 'reparador': 972,\n",
       " 'vermeil': 973,\n",
       " 'brown': 974,\n",
       " 'liquido': 975,\n",
       " 'corrector': 976,\n",
       " 'helar': 977,\n",
       " 'cesta': 978,\n",
       " 'pink': 979,\n",
       " 'canale': 980,\n",
       " 'acondicionador': 981,\n",
       " 'didacticos': 982,\n",
       " 'trenzado': 983,\n",
       " 'ventana': 984,\n",
       " 'vidrio': 985,\n",
       " 'batidora': 986,\n",
       " 'ultimo': 987,\n",
       " 'charol': 988,\n",
       " 'cinco': 989,\n",
       " 'mercedita': 990,\n",
       " 'mum': 991,\n",
       " 'perfil': 992,\n",
       " 'block': 993,\n",
       " 'bag': 994,\n",
       " 'ipad': 995,\n",
       " 'ebooks': 996,\n",
       " 'basica': 997,\n",
       " 'brillante': 998,\n",
       " 'pagina': 999,\n",
       " 'amitie': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    1,    3,   94,\n",
       "       2461, 1432])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word_index has a unique ID assigned to each word in the data. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\t\tid\n",
      "--------------------\n",
      "ropa\t\t34\n",
      "deporte\t\t12\n",
      "abrigo\t\t94\n",
      "raqueta\t\t1645\n",
      "bebe\t\t4\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "test_string = \"ropa deporte abrigo raqueta bebe\"\n",
    "print(\"word\\t\\tid\")\n",
    "print(\"-\" * 20)\n",
    "for word in test_string.split():\n",
    "    print(\"%s\\t\\t%s\" % (word, word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\Enric\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "word_vectors = modelWV.wv\n",
    "vocabulary_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in modelWV:\n",
    "        embedding_matrix[i] = modelWV[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.rand(1, EMBEDDING_DIM)[0]\n",
    "            \n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "embedding_layer = Embedding(input_dim = vocabulary_size,\n",
    "                            output_dim = EMBEDDING_DIM,\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            name='w2v_embedding',\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWV.save(\"w2v_embedding_v1_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_2 = modelWV.wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = word_index\n",
    "#timesteps = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp = Input(shape=(EMBEDDING_DIM, ), name = 'aaa')\n",
    "input_i = Input(shape=(MAX_SEQUENCE_LENGTH, ), name='main_input')\n",
    "embedded_sequences = embedding_layer(input_i)\n",
    "encoded = LSTM(EMBEDDING_DIM, return_sequences=False)(embedded_sequences)\n",
    "\n",
    "decoded = RepeatVector(MAX_SEQUENCE_LENGTH)(encoded)\n",
    "decoded = LSTM(EMBEDDING_DIM, return_sequences=True)(decoded)\n",
    "#decoded = Dense(y_train.shape[1], activation='softmax')(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(input_i, decoded)\n",
    "encoder = Model(input_i, encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(data_vec)\n",
    "X_embedded = encoder.predict(X_train)\n",
    "sequence_autoencoder.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(data_vec)\n",
    "sequence_autoencoder.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "sequence_autoencoder.fit(X_train, X_train,\n",
    "                         epochs=10,\n",
    "                         batch_size=32,\n",
    "                         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204812, 24)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.vstack(data_vec)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.compile('rmsprop', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_embedding (Embedding)    (None, 24, 200)           9883800   \n",
      "=================================================================\n",
      "Total params: 9,883,800\n",
      "Trainable params: 0\n",
      "Non-trainable params: 9,883,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_i = Input(shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "encoded_h1 = Dense(128, activation='relu')(input_i)\n",
    "encoded_h2 = Dense(64, activation='relu')(encoded_h1)\n",
    "encoded_h3 = Dense(32, activation='relu')(encoded_h2)\n",
    "encoded_h4 = Dense(16, activation='relu')(encoded_h3)\n",
    "#encoded_h5 = Dense(8, activation='relu')(encoded_h4)\n",
    "\n",
    "latent = Dense(8, activation='relu', name = 'ENCODER')(encoded_h4)\n",
    "\n",
    "#decoder_h1 = Dense(8, activation='relu')(latent)\n",
    "decoder_h2 = Dense(16, activation='relu')(latent)\n",
    "decoder_h3 = Dense(32, activation='relu')(decoder_h2)\n",
    "decoder_h4 = Dense(64, activation='relu')(decoder_h3)\n",
    "decoder_h5 = Dense(128, activation='relu')(decoder_h4)\n",
    "\n",
    "output = Dense(EMBEDDING_DIM, activation='relu')(decoder_h5)\n",
    "\n",
    "autoencoder = Model(input_i,output)\n",
    "\n",
    "autoencoder.compile('rmsprop','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 24, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 24, 128)           25728     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 24, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 24, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 24, 16)            528       \n",
      "_________________________________________________________________\n",
      "ENCODER (Dense)              (None, 24, 8)             136       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 24, 16)            144       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 24, 32)            544       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 24, 64)            2112      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 24, 128)           8320      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 24, 200)           25800     \n",
      "=================================================================\n",
      "Total params: 73,648\n",
      "Trainable params: 73,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204812/204812 [==============================] - 14s 66us/step\n"
     ]
    }
   ],
   "source": [
    "X_embedded = model.predict(X_train, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "204812/204812 [==============================] - 275s 1ms/step - loss: 0.6771\n",
      "Epoch 2/5\n",
      "204812/204812 [==============================] - 177s 862us/step - loss: 0.6281\n",
      "Epoch 3/5\n",
      "204812/204812 [==============================] - 183s 893us/step - loss: 0.6110\n",
      "Epoch 4/5\n",
      "204812/204812 [==============================] - 180s 879us/step - loss: 0.6037\n",
      "Epoch 5/5\n",
      "204812/204812 [==============================] - 180s 876us/step - loss: 0.5993\n",
      "Wall time: 16min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f69ce874e0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "autoencoder.fit(X_embedded,X_embedded,epochs=5,\n",
    "            batch_size=32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('ENCODER').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('encoder_text_V1.h5')\n",
    "#autoencoder = load_model('autoencoder_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[20000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                     8435465082995\n",
       "brand                                                     Pepe_Jeans\n",
       "text     moda accesorio cartera monedero mujer freida combinado flor\n",
       "Name: 24000, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.loc[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = X_embedded[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204812, 24, 200)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_embedded.copy()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "codes = encoder.predict(X_test)\n",
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24, 8)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_code = encoder.predict(query.reshape(1,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))\n",
    "query_code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204812, 192)\n",
      "(1, 192)\n"
     ]
    }
   ],
   "source": [
    "codes = codes.reshape(-1, 24*8)\n",
    "print(codes.shape)\n",
    "query_code = query_code.reshape(1, 24*8)\n",
    "print(query_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the KNN to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_neigh = 5\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neigh).fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(np.array(query_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 24, 200)\n"
     ]
    }
   ],
   "source": [
    "closest_sent = X_test[indices]\n",
    "closest_sent = closest_sent.reshape(-1,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM); \n",
    "print(closest_sent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the closest text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                     8435465082995\n",
       "brand                                                     Pepe_Jeans\n",
       "text     moda accesorio cartera monedero mujer freida combinado flor\n",
       "Name: 24000, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.loc[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                     8435465082995\n",
      "brand                                                     Pepe_Jeans\n",
      "text     moda accesorio cartera monedero mujer freida combinado flor\n",
      "Name: 24000, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                  8435465082971\n",
      "brand                                                  Pepe_Jeans\n",
      "text     moda accesorio cartera monedero mujer freida solapa flor\n",
      "Name: 23998, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                   8435465075928\n",
      "brand                                                   Pepe_Jeans\n",
      "text     moda accesorio cartera monedero mujer lica combinado azul\n",
      "Name: 24019, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                 1006219124705\n",
      "brand                                                       Tous\n",
      "text     moda accesorio cartera monedero mujer dorp pequeno rosa\n",
      "Name: 29602, dtype: object\n",
      "--------------------------------------------------\n",
      "id                                                 1006219123608\n",
      "brand                                                       Tous\n",
      "text     moda accesorio cartera monedero mujer dorp pequeno azul\n",
      "Name: 29601, dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mis_indices = indices.tolist()[0]\n",
    "for i in range(n_neigh):\n",
    "    print (data_copy.loc[mis_indices[i]])\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204812, 24)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.vstack(data_vec)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 200)\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "flat = GlobalMaxPool1D()(embedded_sequences)\n",
    "print(flat.shape)\n",
    "#Encoder\n",
    "x = Conv1D(50, 3, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(32, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(16, 3, activation='relu')(x)\n",
    "\n",
    "encoded = MaxPooling1D(2, padding='same', name='ENCODER')(x)\n",
    "\n",
    "#Decoder\n",
    "x = Conv1D(16, 3, activation='relu', padding='same')(encoded)\n",
    "x = UpSampling1D(2)(x)\n",
    "x = Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "x = UpSampling1D(2)(x)\n",
    "x = Conv1D(50, 3, activation='relu')(x)\n",
    "x = UpSampling1D(2)(x)\n",
    "decoded = Conv1D(6, 3, activation='sigmoid', padding='same')(x)\n",
    "#decoded = Flatten()(decoded)\n",
    "\n",
    "#preds = Dense(len(MAX_NB_WORDS), activation='softmax')(l_dense)\n",
    "autoencoder = Model(sequence_input, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding (Embedding)    (None, 24, 200)           9883800   \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 22, 50)            30050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 11, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 9, 32)             4832      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 2, 16)             1552      \n",
      "_________________________________________________________________\n",
      "ENCODER (MaxPooling1D)       (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 1, 16)             784       \n",
      "_________________________________________________________________\n",
      "up_sampling1d_44 (UpSampling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 2, 32)             1568      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_45 (UpSampling (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 2, 50)             4850      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_46 (UpSampling (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 4, 6)              906       \n",
      "=================================================================\n",
      "Total params: 9,928,342\n",
      "Trainable params: 44,542\n",
      "Non-trainable params: 9,883,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "204812/204812 [==============================] - 122s 595us/step - loss: 11512867.9217\n",
      "Epoch 2/2\n",
      "204812/204812 [==============================] - 122s 597us/step - loss: 11512866.7484\n",
      "Wall time: 4min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2360fd289e8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "autoencoder.fit(X_train, X_train, epochs=2, batch_size=32, \n",
    "                callbacks=None, verbose = 1, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V Buena LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(data_vec)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(data_vec)\n",
    "X_train[0]\n",
    "#sequence = X_train[0].reshape((1, MAX_SEQUENCE_LENGTH))\n",
    "#sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "embedded_sequences = embedding_layer(inputs)\n",
    "encoded = LSTM(EMBEDDING_DIM, return_sequences=False)(embedded_sequences)\n",
    "\n",
    "decoded = RepeatVector(MAX_SEQUENCE_LENGTH)(encoded)\n",
    "decoded = LSTM(MAX_SEQUENCE_LENGTH, return_sequences=False)(decoded)\n",
    "#decoded = TimeDistributed(Dense(MAX_SEQUENCE_LENGTH))(decoded)\n",
    "\n",
    "encoder = Model(inputs, encoded)\n",
    "#sequence_autoencoder = Model(inputs, decoded)\n",
    "decoder = Model(inputs, decoded)\n",
    "autoencoder = Model(encoder.inputs, decoder(encoder.inputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding (Embedding)    (None, 23, 200)           9883800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               320800    \n",
      "=================================================================\n",
      "Total params: 10,204,600\n",
      "Trainable params: 320,800\n",
      "Non-trainable params: 9,883,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****************************************************************\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding (Embedding)    (None, 23, 200)           9883800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 23, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 23)                20608     \n",
      "=================================================================\n",
      "Total params: 10,225,208\n",
      "Trainable params: 341,408\n",
      "Non-trainable params: 9,883,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "*****************************************************************\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 23)                10225208  \n",
      "=================================================================\n",
      "Total params: 10,225,208\n",
      "Trainable params: 341,408\n",
      "Non-trainable params: 9,883,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (encoder.summary())\n",
    "print('*'*65)\n",
    "print(decoder.summary())\n",
    "print('*'*65)\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='mse',\n",
    "              optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "204812/204812 [==============================] - 736s 4ms/step - loss: 12013427.5849\n",
      "Epoch 2/5\n",
      "204812/204812 [==============================] - 725s 4ms/step - loss: 12013426.0270\n",
      "Epoch 3/5\n",
      "204812/204812 [==============================] - 731s 4ms/step - loss: 12013425.9610\n",
      "Epoch 4/5\n",
      "186048/204812 [==========================>...] - ETA: 1:06 - loss: 12025301.6378"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-19464a80452f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           verbose = 1)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23584/204812 [==>...........................] - ETA: 3:03"
     ]
    }
   ],
   "source": [
    "encoderPredictions = encoder.predict(X_train, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderPredictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(encoder.inputs, sequence_autoencoder(encoder(encoder.inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(sequence, verbose=0)\n",
    "print(yhat[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('text_autoencoder_v1.h5')\n",
    "#autoencoder = load_model('text_autoencoder_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[49800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
