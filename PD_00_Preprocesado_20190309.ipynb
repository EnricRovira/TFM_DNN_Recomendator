{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.kdnuggets.com/wp-content/uploads/text-preprocessing-framework-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\aeroengy\\anaconda3\\lib\\site-packages (0.0.17)\n"
     ]
    }
   ],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in c:\\users\\aeroengy\\anaconda3\\lib\\site-packages (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\aeroengy\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import contractions\n",
    "import inflect\n",
    "import re, unicodedata\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "\n",
    "#import my_functions\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_ESP = list(set(stopwords.words('spanish')))\n",
    "STOPWORDS_CAT = open('stopwords_catalan.txt', 'r', encoding= 'UTF-8').read().split()\n",
    "STOPWORDS_ENG = list(set(stopwords.words('english')))\n",
    "CUSTOM  = ['uf', '100st', '100t', '100u', 'pal', 'zzb21601xu', 'x2']\n",
    "\n",
    "STOPWORDS_ALL = STOPWORDS_ESP + STOPWORDS_CAT + STOPWORDS_ENG + CUSTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file download with wget module\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-es.txt'  \n",
    "#wget.download(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = {}\n",
    "with open(\"lemmatization-es.txt\", encoding= 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            (key, val) = line.split()\n",
    "        except:\n",
    "            pass\n",
    "        lemmatizer[str(key)] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_inv = {v: k for k, v in lemmatizer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../02_Comprension de Datos/Descripciones204k_20190315.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo masculino con textura de mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                                          text  \n",
       "0                     Moda Mujer Abrigos Abrigo masculino con textura de mujer  \n",
       "1             Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono  \n",
       "2   Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés  \n",
       "3  Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos  \n",
       "4            Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una lista de marcas para poder quitarlas del texto en caso de que aparezcan, eliminamos las marcas duplicadas para comprimir el tamaño de la lista y optimizar el proceso de bsuqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "copia = data.copy()\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def normalize_brands(words):\n",
    "    words = str(words)\n",
    "    words  = words.strip().split()\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)    \n",
    "    words = list(dict.fromkeys(words)) #Remove duplicates\n",
    "    words = \" \".join(words)\n",
    "    return words\n",
    "    \n",
    "copia['brand'] = copia['brand'].apply(normalize_brands)\n",
    "brands = list(dict.fromkeys(copia['brand'].values))\n",
    "brands.remove('marvel')\n",
    "more_brands = ['tarocco', 'tucci', 'fjord', 'tommy', 'hilfiger', 'easy', 'wear', 'josma']\n",
    "for brand in more_brands:\n",
    "    brands.append(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5088"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la normalización vamos a realizar una serie de tareas destinadas a poner todo el texto en un campo de juego nivelado: convertir todo el texto a la misma mayúscula o minúscula, eliminar la puntuación, convertir las cifras a sus equivalentes en palabras, etc. La normalización pone todas las palabras en pie de igualdad y permite que el procesamiento se realice de manera uniforme. Algunas de las técnicas que vamos a aplicar son:\n",
    "\n",
    "- Borrar caracteres extraños\n",
    "- Pasar todo el texto a minusculas\n",
    "- Quitar simbolos de puntuacion ( . , &, !, ?, ¿, /, etc)\n",
    "- Pasar numeros en digito a texto\n",
    "- Quitar stopwords\n",
    "- Stemming\n",
    "- Lexematizar\n",
    "\n",
    "**Importante:** Despues de la Nomralizacion trabajaremos a nivel de palabra o token en vez de a nivel de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ['corte ingles', 'bob esponja', 'color azul', 'cristiano ronaldo', 'fc barcelona', 'leo messi', 'real madrid',\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        #new_word = unicodedata.normalize('NFD', word).encode('ascii', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if not word.isdigit():\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORDS_ALL and len(word) > 1:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def remove_brands(words):\n",
    "    \"\"\"Remove brands from the text\"\"\"\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        if word not in brands:\n",
    "            clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "def lemmatize(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        if word in lemmatizer_inv:\n",
    "            lemmas.append(lemmatizer_inv[word])\n",
    "        else: lemmas.append(word)\n",
    "    return lemmas\n",
    "\n",
    "def replace_ngrams(words):\n",
    "    for gram in ngrams:\n",
    "        if (gram in words):\n",
    "            g = \"_\".join(gram.split())\n",
    "            words = words.replace(gram, g)\n",
    "    return words\n",
    "\n",
    "def normalize(words):\n",
    "    #if pandas == True:\n",
    "    words  = words.strip().split()\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_brands(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = list(dict.fromkeys(words)) #Remove duplicates\n",
    "    words = \" \".join(words)\n",
    "    words = replace_ngrams(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#4min 4sec\n",
    "data_copy['text'] = data_copy['text'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo masculino textura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo doble faz cinturon tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>moda mujer abrigo largo antelina woman corte_ingles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>moda mujer abrigo chaqueta termica efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>moda mujer abrigo parka algodon capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                     text  \n",
       "0                     moda mujer abrigo masculino textura  \n",
       "1               moda mujer abrigo doble faz cinturon tono  \n",
       "2     moda mujer abrigo largo antelina woman corte_ingles  \n",
       "3  moda mujer abrigo chaqueta termica efecto cortavientos  \n",
       "4                 moda mujer abrigo parka algodon capucha  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1060651400131</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo masculino con textura de mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1060651400180</td>\n",
       "      <td>Woman_Limited_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051056400107</td>\n",
       "      <td>Woman_El_Corte_Inglés</td>\n",
       "      <td>Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1019350401147</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019353400229</td>\n",
       "      <td>Lloyd's</td>\n",
       "      <td>Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                          brand  \\\n",
       "0  1060651400131  Woman_Limited_El_Corte_Inglés   \n",
       "1  1060651400180  Woman_Limited_El_Corte_Inglés   \n",
       "2  1051056400107          Woman_El_Corte_Inglés   \n",
       "3  1019350401147                        Lloyd's   \n",
       "4  1019353400229                        Lloyd's   \n",
       "\n",
       "                                                                          text  \n",
       "0                     Moda Mujer Abrigos Abrigo masculino con textura de mujer  \n",
       "1             Moda Mujer Abrigos Abrigo doble faz de mujer con cinturón a tono  \n",
       "2   Moda Mujer Abrigos Abrigo largo de antelina de mujer Woman El Corte Inglés  \n",
       "3  Moda Mujer Abrigos Chaqueta térmica de mujer Lloyds con efecto cortavientos  \n",
       "4            Moda Mujer Abrigos Parka 100% algodón de mujer Lloyds con capucha  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                      1005112393706\n",
       "brand                                                          ISLAND\n",
       "text     musica dance electronica ambient chill omen remixes lpvinilo\n",
       "Name: 200000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7000\n",
    "data_copy.loc[200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204812,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max words in a sentence:          Mean words in a sentence:\n",
      "------------------------------------------------------------\n",
      "23                                8.072564107571822\n",
      "------------------------------------------------------------\n",
      "libro salud bienestar dieta nutricion general detox sen sano dentro bello clave nutricional rutina diarias eliminar toxina forma saludable energetica nutritiva tapa blanda\n"
     ]
    }
   ],
   "source": [
    "maxw = 0\n",
    "mean = 0\n",
    "max_s = 0\n",
    "for i, sentence in enumerate(data_copy['text']):\n",
    "    mean += len(sentence.split())\n",
    "    if maxw < len(sentence.split()):\n",
    "        maxw = len(sentence.split())\n",
    "        max_s = i\n",
    "print('Max words in a sentence:' + ' '*10 + 'Mean words in a sentence:')\n",
    "print(\"-\" * 60)\n",
    "print (f\"{maxw} {' '*30} {mean/len(data_copy['text'])}\")\n",
    "print(\"-\" * 60)\n",
    "print (data_copy['text'][max_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.to_csv('Texto_PreProcesado.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramas (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'moda', 'mujer', 'abrigo', 'masculino', 'textura', '1', 'doble', 'faz', 'cinturon']\n",
      "257052\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = nltk.word_tokenize(data_copy['text'].to_string())\n",
    "bag_of_words = list(dict.fromkeys(bag_of_words))\n",
    "print(bag_of_words[:10])\n",
    "print(len(bag_of_words)) \n",
    "#126k productos = 2_063_845 palabras\n",
    "#175k productos = 2_838_097 palabras\n",
    "#204k productos = 3_070_773 palabras -> 1_878_308 20/03/2019 -> no dups = 257042, ngrams =257044 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moda mujer abrigo masculino textura',\n",
       " 'moda mujer abrigo doble faz cinturon tono',\n",
       " 'moda mujer abrigo largo antelina woman corte_ingles',\n",
       " 'moda mujer abrigo chaqueta termica efecto cortavientos',\n",
       " 'moda mujer abrigo parka algodon capucha']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [sent for sent in data_copy['text']]\n",
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moda',\n",
       " 'mujer',\n",
       " 'abrigo',\n",
       " 'masculino',\n",
       " 'textura',\n",
       " 'moda mujer',\n",
       " 'mujer abrigo',\n",
       " 'abrigo masculino',\n",
       " 'masculino textura',\n",
       " 'doble']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.keys())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "#pprint.pprint (vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['moda', 'mujer', 'abrigo', 'masculino', 'textura'],\n",
       " ['moda', 'mujer', 'abrigo', 'doble', 'faz', 'cinturon', 'tono'],\n",
       " ['moda', 'mujer', 'abrigo', 'largo', 'antelina', 'woman', 'corte_ingles']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [word.split() for word in data_copy['text'].values]\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204812"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(sentences, workers = 3, min_count=5, window = 10, size = 200)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"word2vec_model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baloncesto', 0.44440093636512756),\n",
       " ('fc_barcelona', 0.4238649010658264),\n",
       " ('madrid', 0.39621978998184204),\n",
       " ('futboltaco', 0.3959360122680664),\n",
       " ('cf', 0.3918527364730835),\n",
       " ('equipacion', 0.3738066852092743),\n",
       " ('atletico', 0.370324969291687),\n",
       " ('escudo', 0.3675953447818756),\n",
       " ('real_madrid', 0.3640595078468323),\n",
       " ('turf', 0.3561278283596039)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Word2Vec.load(\"word2vec_model_v1\")\n",
    "wl = 'futbol'\n",
    "model.wv.most_similar (positive = wl)\n",
    "#model.wv.most_similar_cosmul(positive = ['disfraz', 'abrigo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carrycot', 0.4795910120010376),\n",
       " ('cambiador', 0.4572114944458008),\n",
       " ('portabebes', 0.44224661588668823),\n",
       " ('maternal', 0.43625256419181824),\n",
       " ('trona', 0.4313957989215851),\n",
       " ('duet', 0.4276811182498932),\n",
       " ('auto', 0.4258055090904236),\n",
       " ('capazo', 0.425559937953949),\n",
       " ('isofix', 0.42419910430908203),\n",
       " ('modular', 0.42381948232650757)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similar_products(text):\n",
    "    text = normalize(text)\n",
    "    list_text = text.split()\n",
    "    most_similar = model.wv.most_similar_cosmul(positive = list_text)\n",
    "    \n",
    "    return most_similar\n",
    "    \n",
    "similar_products('Silla de paseo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Sentences\n",
    "\n",
    "- Initialize tokenizer with num_words = MAX_NB_WORDS (200K). i.e. The tokenizer will perform a word count, sorted by number of occurences in descending order and pick top N words, 200K in this case \n",
    "- Use tokenizer's texts_to_sequences method to convert text to array of integers.\n",
    "- The arrays obtained from previous step might not be of uniform length, use pad_sequences method to obtain arrays with length equal to MAX_SEQUENCE_LENGTH (30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 257053 #200k\n",
    "MAX_SEQUENCE_LENGTH = 23                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = data_copy['text']\n",
    "all_text = all_text.drop_duplicates (keep = False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "data_sequences = tokenizer.texts_to_sequences(data_copy['text'])\n",
    "data_vec = pad_sequences(data_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49422 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,    3,   94, 2461,\n",
       "       1432])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word_index has a unique ID assigned to each word in the data. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\t\tid\n",
      "--------------------\n",
      "ropa\t\t34\n",
      "deporte\t\t12\n",
      "abrigo\t\t94\n",
      "raqueta\t\t1644\n",
      "bebe\t\t4\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "test_string = \"ropa deporte abrigo raqueta bebe\"\n",
    "print(\"word\\t\\tid\")\n",
    "print(\"-\" * 20)\n",
    "for word in test_string.split():\n",
    "    print(\"%s\\t\\t%s\" % (word, word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "vocabulary_size = min(len(word_index)+1,MAX_NB_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "embedding_layer = Embedding(input_dim = vocabulary_size,\n",
    "                            output_dim = EMBEDDING_DIM,\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            name='w2v_embedding',\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"w2v_embedding_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "x = embedding_layer(i)\n",
    "x = Flatten()(x)\n",
    "o = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "w2v_embedding (Embedding)    (None, 23, 200)           9884600   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4601      \n",
      "=================================================================\n",
      "Total params: 9,889,201\n",
      "Trainable params: 4,601\n",
      "Non-trainable params: 9,884,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
